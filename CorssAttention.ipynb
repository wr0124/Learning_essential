{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6q072RjMbwA6jBrIaMCsG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wr0124/Learning_essential/blob/main/CorssAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#source\n",
        "\n",
        "old: https://github.com/huggingface/diffusers_all/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "new: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py#L41\n",
        "\n",
        "https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "branch: https://github.com/huggingface/diffusers/blob/4125756e88e82370c197fecf28e9f0b4d7eee6c3/src/diffusers/models/cross_attention.py\n",
        "\n",
        "other:\n",
        "show attention: https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_ldm.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "BIzPJV4YjsGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "SZuCq7aw0CDr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rwbzpXtUV85e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.self_attention"
      ],
      "metadata": {
        "id": "R_4jnLicHQCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1alg"
      ],
      "metadata": {
        "id": "XVp7sdZnF2tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "        #print(\"W_query shape:\", self.W_query)\n",
        "        #print(\"W_key shape:\", self.W_key)\n",
        "        #print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        #print(\"keys \", keys)\n",
        "        #print(\"queries \", queries)\n",
        "        #print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "EE1cZUpZ3RtG"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for MultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "batch_size = 1\n",
        "seq_len = 6\n",
        "\n",
        "multi_head_attention = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, sequence_length, d)).float()\n",
        "#print(\"MultiHeadAttentionWrapper input shape:\", x)\n",
        "output = multi_head_attention(x)\n",
        "#print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n"
      ],
      "metadata": {
        "id": "QKmBdqOu33_8",
        "outputId": "767fde1d-ca82-47ba-8214-9c522fdce4c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(A)\n",
        "print(A.shape)\n",
        "B=torch.tensor([[4,5,6],[7,8,9]])\n",
        "print(B)\n",
        "print(B.shape)\n",
        "print(torch.cat((A,B),dim=-1))"
      ],
      "metadata": {
        "id": "BFSHwJqbxMhW",
        "outputId": "644fad90-82cf-4356-fe06-477eb7e890e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[1, 2, 3, 4, 5, 6],\n",
            "        [4, 5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2two methods compare"
      ],
      "metadata": {
        "id": "g8eiLuUBF-uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the selfttention class from Method 1\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "        print(\"W_query shape:\", self.W_query)\n",
        "        print(\"W_key shape:\", self.W_key)\n",
        "        print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        #print(\"keys \", keys)\n",
        "        #print(\"queries \", queries)\n",
        "        #print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        #print(\"heads \", self.heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# Example input\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input shape:\", x)\n",
        "\n",
        "# Initialize Method 1\n",
        "multihead_attention_1 = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output_1 = multihead_attention_1(x)\n",
        "\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x @ head.W_query for head in multihead_attention_1.heads]\n",
        "keys = [x @ head.W_key for head in multihead_attention_1.heads]\n",
        "values = [x @ head.W_value for head in multihead_attention_1.heads]\n",
        "print(\"queries before cat \", queries)\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1) )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv  )\n",
        "\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "qkv_attention = QKVAttention(num_heads)\n",
        "output_2 = qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1, output_1.shape)\n",
        "print(\"Output from Method 2:\\n\", output_2, output_2.shape)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "id": "gyZzKWxwBd-Y",
        "outputId": "7811004d-2190-481d-cea8-7063f50decc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttentionWrapper input shape: tensor([[[2., 0., 2., 0.],\n",
            "         [0., 0., 1., 1.],\n",
            "         [0., 1., 0., 1.],\n",
            "         [3., 2., 3., 2.],\n",
            "         [3., 2., 2., 3.],\n",
            "         [0., 2., 3., 3.]]])\n",
            "W_query shape: Parameter containing:\n",
            "tensor([[3., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 2.],\n",
            "        [3., 3.]], requires_grad=True)\n",
            "W_key shape: Parameter containing:\n",
            "tensor([[2., 2.],\n",
            "        [1., 3.],\n",
            "        [3., 0.],\n",
            "        [0., 3.]], requires_grad=True)\n",
            "W_value shape: Parameter containing:\n",
            "tensor([[0., 1.],\n",
            "        [3., 3.],\n",
            "        [3., 3.],\n",
            "        [3., 2.]], requires_grad=True)\n",
            "W_query shape: Parameter containing:\n",
            "tensor([[1., 1.],\n",
            "        [2., 1.],\n",
            "        [1., 2.],\n",
            "        [2., 0.]], requires_grad=True)\n",
            "W_key shape: Parameter containing:\n",
            "tensor([[1., 3.],\n",
            "        [0., 1.],\n",
            "        [0., 3.],\n",
            "        [0., 1.]], requires_grad=True)\n",
            "W_value shape: Parameter containing:\n",
            "tensor([[2., 1.],\n",
            "        [2., 1.],\n",
            "        [2., 3.],\n",
            "        [3., 2.]], requires_grad=True)\n",
            "queries before cat  [tensor([[[ 8.,  6.],\n",
            "         [ 4.,  5.],\n",
            "         [ 4.,  3.],\n",
            "         [20., 15.],\n",
            "         [22., 16.],\n",
            "         [14., 15.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[ 4.,  6.],\n",
            "         [ 3.,  2.],\n",
            "         [ 4.,  1.],\n",
            "         [14., 11.],\n",
            "         [15.,  9.],\n",
            "         [13.,  8.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "queries after cat tensor([[[ 8.,  6.,  4.,  6.],\n",
            "         [ 4.,  5.,  3.,  2.],\n",
            "         [ 4.,  3.,  4.,  1.],\n",
            "         [20., 15., 14., 11.],\n",
            "         [22., 16., 15.,  9.],\n",
            "         [14., 15., 13.,  8.]]], grad_fn=<CatBackward0>)\n",
            "before permute qkv tensor([[[ 8.,  6.,  4.,  6., 10.,  4.,  2., 12.,  6.,  8.,  8.,  8.],\n",
            "         [ 4.,  5.,  3.,  2.,  3.,  3.,  0.,  4.,  6.,  5.,  5.,  5.],\n",
            "         [ 4.,  3.,  4.,  1.,  1.,  6.,  0.,  2.,  6.,  5.,  5.,  3.],\n",
            "         [20., 15., 14., 11., 17., 18.,  3., 22., 21., 22., 22., 18.],\n",
            "         [22., 16., 15.,  9., 14., 21.,  3., 20., 21., 21., 23., 17.],\n",
            "         [14., 15., 13.,  8., 11., 15.,  0., 14., 24., 21., 19., 17.]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "after permute qkv tensor([[[ 8.,  4.,  4., 20., 22., 14.],\n",
            "         [ 6.,  5.,  3., 15., 16., 15.],\n",
            "         [ 4.,  3.,  4., 14., 15., 13.],\n",
            "         [ 6.,  2.,  1., 11.,  9.,  8.],\n",
            "         [10.,  3.,  1., 17., 14., 11.],\n",
            "         [ 4.,  3.,  6., 18., 21., 15.],\n",
            "         [ 2.,  0.,  0.,  3.,  3.,  0.],\n",
            "         [12.,  4.,  2., 22., 20., 14.],\n",
            "         [ 6.,  6.,  6., 21., 21., 24.],\n",
            "         [ 8.,  5.,  5., 22., 21., 21.],\n",
            "         [ 8.,  5.,  5., 22., 23., 19.],\n",
            "         [ 8.,  5.,  3., 18., 17., 17.]]], grad_fn=<PermuteBackward0>)\n",
            "Output from Method 1:\n",
            " tensor([[[21.0000, 21.0000, 21.0000, 21.0000, 21.0000, 21.0000],\n",
            "         [21.9858, 21.1070, 21.8930, 22.0000, 22.0000, 21.1070],\n",
            "         [22.0002, 22.0558, 22.1950, 22.0000, 22.0000, 22.0000],\n",
            "         [17.9998, 17.9442, 17.8040, 18.0000, 18.0000, 18.0000]]],\n",
            "       grad_fn=<PermuteBackward0>) torch.Size([1, 4, 6])\n",
            "Output from Method 2:\n",
            " tensor([[[21.0000, 21.0000, 21.0000, 21.0000, 21.0000, 21.0000],\n",
            "         [21.9858, 21.1070, 21.8930, 22.0000, 22.0000, 21.1070],\n",
            "         [22.0002, 22.0558, 22.1950, 22.0000, 22.0000, 22.0000],\n",
            "         [17.9998, 17.9442, 17.8040, 18.0000, 18.0000, 18.0000]]],\n",
            "       grad_fn=<UnsafeViewBackward0>) torch.Size([1, 4, 6])\n",
            "Are the outputs close?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3attention_map\n",
        "https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb"
      ],
      "metadata": {
        "id": "om-WM61h0Bfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.CrossAttention"
      ],
      "metadata": {
        "id": "dJRtOrM46kkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1alg"
      ],
      "metadata": {
        "id": "D2JBgzJNHJvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "2lCfxcpB7smg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for CrossMultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "batch_size = 1\n",
        "seq_len = 6\n",
        "\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input1 shape:\", x_1)\n",
        "print(\"MultiHeadAttentionWrapper input2 shape:\", x_2)\n",
        "output = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n",
        "\n"
      ],
      "metadata": {
        "id": "aZcqEqSWIxep",
        "outputId": "31584e3c-f3e9-489c-c19d-222d908a8ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper input1 shape: tensor([[[0., 1., 1.],\n",
            "         [1., 1., 2.],\n",
            "         [2., 0., 2.],\n",
            "         [0., 2., 1.],\n",
            "         [2., 1., 1.],\n",
            "         [2., 1., 0.]]])\n",
            "MultiHeadAttentionWrapper input2 shape: tensor([[[1., 1., 2.],\n",
            "         [1., 1., 1.],\n",
            "         [0., 2., 0.],\n",
            "         [0., 1., 1.],\n",
            "         [0., 2., 2.],\n",
            "         [0., 0., 2.]]])\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[3.3459, 7.3445, 4.0000, 4.0000],\n",
            "         [3.9441, 7.9441, 4.0000, 4.0000],\n",
            "         [3.9861, 7.9861, 4.0000, 4.0000],\n",
            "         [3.3457, 7.3454, 4.0000, 4.0000],\n",
            "         [3.9441, 7.9441, 4.0000, 4.0000],\n",
            "         [3.7887, 7.7870, 3.9999, 3.9999]]], grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 two methods compare"
      ],
      "metadata": {
        "id": "BZcr1G8ZKe2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the QKVAttention class from Method 1\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class CrossQKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# Example input\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"CrossMultiHeadAttentionWrapper input 1 shape:\", x_1)\n",
        "print(\"CrossMultiHeadAttentionWrapper input 2 shape:\", x_2)\n",
        "\n",
        "# Initialize Method 1\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "\n",
        "print(\"-----------------------------------------------------\")\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x_1 @ head.W_query for head in cross_multi_head_attention.heads]\n",
        "keys = [x_2 @ head.W_key for head in cross_multi_head_attention.heads]\n",
        "values = [x_2 @ head.W_value for head in cross_multi_head_attention.heads]\n",
        "print(\"queries before cat \", queries)\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1) )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv  )\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "qkv_attention = QKVAttention(num_heads)\n",
        "output_2 = qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1)\n",
        "print(\"Output from Method 2:\\n\", output_2)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "#print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "id": "_oxGKS6nKmOb",
        "outputId": "89ab6d21-924e-4e0a-a40c-b34f413694ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossMultiHeadAttentionWrapper input 1 shape: tensor([[[2., 0., 3., 2.],\n",
            "         [3., 1., 0., 1.],\n",
            "         [3., 3., 0., 3.],\n",
            "         [2., 1., 2., 3.],\n",
            "         [2., 3., 1., 1.],\n",
            "         [3., 3., 3., 3.]]])\n",
            "CrossMultiHeadAttentionWrapper input 2 shape: tensor([[[0., 2., 0., 0.],\n",
            "         [0., 2., 2., 1.],\n",
            "         [3., 3., 2., 2.],\n",
            "         [2., 3., 3., 2.],\n",
            "         [3., 0., 3., 2.],\n",
            "         [0., 0., 0., 3.]]])\n",
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[14.9442, 25.0000, 22.5000, 15.5000],\n",
            "         [14.9858, 25.0000, 22.4999, 15.5000],\n",
            "         [14.9858, 25.0000, 22.5000, 15.5000],\n",
            "         [14.9442, 25.0000, 22.5000, 15.5000],\n",
            "         [14.9442, 25.0000, 22.5000, 15.5000],\n",
            "         [14.9858, 25.0000, 22.5000, 15.5000]]], grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n",
            "-----------------------------------------------------\n",
            "queries before cat  [tensor([[[15., 19.],\n",
            "         [ 6., 12.],\n",
            "         [12., 18.],\n",
            "         [15., 19.],\n",
            "         [10., 14.],\n",
            "         [21., 27.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[17., 12.],\n",
            "         [ 9.,  3.],\n",
            "         [21.,  9.],\n",
            "         [20., 13.],\n",
            "         [17.,  5.],\n",
            "         [30., 15.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "queries after cat tensor([[[15., 19., 17., 12.],\n",
            "         [ 6., 12.,  9.,  3.],\n",
            "         [12., 18., 21.,  9.],\n",
            "         [15., 19., 20., 13.],\n",
            "         [10., 14., 17.,  5.],\n",
            "         [21., 27., 30., 15.]]], grad_fn=<CatBackward0>)\n",
            "before permute qkv tensor([[[15., 19., 17., 12.,  2.,  4.,  2.,  0.,  0.,  4.,  6.,  2.],\n",
            "         [ 6., 12.,  9.,  3.,  9.,  7.,  6.,  3.,  7., 12.,  9.,  6.],\n",
            "         [12., 18., 21.,  9., 22., 10., 12.,  7., 14., 25., 24., 16.],\n",
            "         [15., 19., 20., 13., 21., 11., 12.,  7., 15., 25., 21., 15.],\n",
            "         [10., 14., 17.,  5., 21.,  5., 10.,  8., 17., 22., 15., 15.],\n",
            "         [21., 27., 30., 15.,  9.,  3.,  6.,  3.,  3.,  6.,  9.,  0.]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "after permute qkv tensor([[[15.,  6., 12., 15., 10., 21.],\n",
            "         [19., 12., 18., 19., 14., 27.],\n",
            "         [17.,  9., 21., 20., 17., 30.],\n",
            "         [12.,  3.,  9., 13.,  5., 15.],\n",
            "         [ 2.,  9., 22., 21., 21.,  9.],\n",
            "         [ 4.,  7., 10., 11.,  5.,  3.],\n",
            "         [ 2.,  6., 12., 12., 10.,  6.],\n",
            "         [ 0.,  3.,  7.,  7.,  8.,  3.],\n",
            "         [ 0.,  7., 14., 15., 17.,  3.],\n",
            "         [ 4., 12., 25., 25., 22.,  6.],\n",
            "         [ 6.,  9., 24., 21., 15.,  9.],\n",
            "         [ 2.,  6., 16., 15., 15.,  0.]]], grad_fn=<PermuteBackward0>)\n",
            "Output from Method 1:\n",
            " tensor([[[21.0000, 21.9858, 22.0002, 17.9998],\n",
            "         [21.0000, 21.1070, 22.0558, 17.9442],\n",
            "         [21.0000, 21.8930, 22.1950, 17.8040],\n",
            "         [21.0000, 22.0000, 22.0000, 18.0000],\n",
            "         [21.0000, 22.0000, 22.0000, 18.0000],\n",
            "         [21.0000, 21.1070, 22.0000, 18.0000]]], grad_fn=<PermuteBackward0>)\n",
            "Output from Method 2:\n",
            " tensor([[[14.9442, 14.9858, 14.9858, 14.9442, 14.9442, 14.9858],\n",
            "         [25.0000, 25.0000, 25.0000, 25.0000, 25.0000, 25.0000],\n",
            "         [22.5000, 22.4999, 22.5000, 22.5000, 22.5000, 22.5000],\n",
            "         [15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-749101dd31ab>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Check if the outputs are close enough\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Are the outputs close? \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 2"
          ]
        }
      ]
    }
  ]
}