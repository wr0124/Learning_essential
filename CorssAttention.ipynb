{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTdPY79GB6NnJpMnac0/hh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wr0124/Learning_essential/blob/main/CorssAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#source\n",
        "\n",
        "\"self_attention\"\n",
        "old: https://github.com/huggingface/diffusers_all/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "new: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py#L41\n",
        "\n",
        "https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "branch: https://github.com/huggingface/diffusers/blob/4125756e88e82370c197fecf28e9f0b4d7eee6c3/src/diffusers/models/cross_attention.py\n",
        "\n",
        "https://github.com/openai/consistency_models/blob/e32b69ee436d518377db86fb2127a3972d0d8716/cm/unet.py#L478\n",
        "\n",
        "https://github.com/openai/guided-diffusion/blob/22e0df8183507e13a7813f8d38d51b072ca1e67c/guided_diffusion/unet.py#L361\n",
        "\n",
        "\n",
        "other:\n",
        "show attention: https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_ldm.ipynb\n",
        "\n",
        "\"cross_attention\"\n",
        "\n",
        "https://github.com/openai/point-e/blob/fc8a607c08a3ea804cc82bf1ef8628f88a3a5d2f/point_e/models/perceiver.py#L11\n",
        "\n",
        "https://github.com/openai/shap-e/blob/50131012ee11c9d2617f3886c10f000d3c7a3b43/shap_e/models/generation/perceiver.py#L13\n",
        "\n",
        "https://github.com/CompVis/latent-diffusion/blob/a506df5756472e2ebaf9078affdde2c4f1502cd4/ldm/modules/attention.py#L152\n",
        "\n",
        "https://blog.csdn.net/weixin_54338498/article/details/135051918\n",
        "\n",
        "\"attention_map\"\n",
        "https://github.com/google/prompt-to-prompt/tree/main\n"
      ],
      "metadata": {
        "id": "BIzPJV4YjsGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From animateddiff lib\n",
        "juliew@neptune7:~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/models"
      ],
      "metadata": {
        "id": "-HjxibZEvDiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ori"
      ],
      "metadata": {
        "id": "p4v7XLpNrNpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feedforward\n",
        "class GEGLU(nn.Module):\n",
        "    r\"\"\"\n",
        "    A variant of the gated linear unit activation function from https://arxiv.org/abs/2002.05202.\n",
        "\n",
        "    Parameters:\n",
        "        dim_in (`int`): The number of channels in the input.\n",
        "        dim_out (`int`): The number of channels in the output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_in: int, dim_out: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def gelu(self, gate):\n",
        "        if gate.device.type != \"mps\":\n",
        "            return F.gelu(gate)\n",
        "        # mps: gelu is not implemented for float16\n",
        "        return F.gelu(gate.to(dtype=torch.float32)).to(dtype=gate.dtype)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
        "        return hidden_states * self.gelu(gate)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    r\"\"\"\n",
        "    A feed-forward layer.\n",
        "\n",
        "    Parameters:\n",
        "        dim (`int`): The number of channels in the input.\n",
        "        dim_out (`int`, *optional*): The number of channels in the output. If not given, defaults to `dim`.\n",
        "        mult (`int`, *optional*, defaults to 4): The multiplier to use for the hidden dimension.\n",
        "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
        "        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        dim_out: Optional[int] = None,\n",
        "        mult: int = 4,\n",
        "        dropout: float = 0.0,\n",
        "        activation_fn: str = \"geglu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = dim_out if dim_out is not None else dim\n",
        "\n",
        "        if activation_fn == \"gelu\":\n",
        "            act_fn = GELU(dim, inner_dim)\n",
        "        elif activation_fn == \"geglu\":\n",
        "            act_fn = GEGLU(dim, inner_dim)\n",
        "        elif activation_fn == \"geglu-approximate\":\n",
        "            act_fn = ApproximateGELU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.ModuleList([])\n",
        "        # project in\n",
        "        self.net.append(act_fn)\n",
        "        # project dropout\n",
        "        self.net.append(nn.Dropout(dropout))\n",
        "        # project out\n",
        "        self.net.append(nn.Linear(inner_dim, dim_out))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for module in self.net:\n",
        "            hidden_states = module(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    r\"\"\"\n",
        "    A cross attention layer.\n",
        "\n",
        "    Parameters:\n",
        "        query_dim (`int`): The number of channels in the query.\n",
        "        cross_attention_dim (`int`, *optional*):\n",
        "            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n",
        "        heads (`int`,  *optional*, defaults to 8): The number of heads to use for multi-head attention.\n",
        "        dim_head (`int`,  *optional*, defaults to 64): The number of channels in each head.\n",
        "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
        "        bias (`bool`, *optional*, defaults to False):\n",
        "            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_dim: int,\n",
        "        cross_attention_dim: Optional[int] = None,\n",
        "        heads: int = 8,\n",
        "        dim_head: int = 64,\n",
        "        dropout: float = 0.0,\n",
        "        bias=False,\n",
        "        upcast_attention: bool = False,\n",
        "        upcast_softmax: bool = False,\n",
        "        added_kv_proj_dim: Optional[int] = None,\n",
        "        norm_num_groups: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        cross_attention_dim = cross_attention_dim if cross_attention_dim is not None else query_dim\n",
        "        self.upcast_attention = upcast_attention\n",
        "        self.upcast_softmax = upcast_softmax\n",
        "\n",
        "        self.scale = dim_head**-0.5\n",
        "\n",
        "        self.heads = heads\n",
        "        # for slice_size > 0 the attention score computation\n",
        "        # is split across the batch axis to save memory\n",
        "        # You can set slice_size with `set_attention_slice`\n",
        "        self.sliceable_head_dim = heads\n",
        "        self._slice_size = None\n",
        "        self._use_memory_efficient_attention_xformers = False\n",
        "        self.added_kv_proj_dim = added_kv_proj_dim\n",
        "\n",
        "        if norm_num_groups is not None:\n",
        "            self.group_norm = nn.GroupNorm(num_channels=inner_dim, num_groups=norm_num_groups, eps=1e-5, affine=True)\n",
        "        else:\n",
        "            self.group_norm = None\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=bias)\n",
        "        self.to_k = nn.Linear(cross_attention_dim, inner_dim, bias=bias)\n",
        "        self.to_v = nn.Linear(cross_attention_dim, inner_dim, bias=bias)\n",
        "\n",
        "        if self.added_kv_proj_dim is not None:\n",
        "            self.add_k_proj = nn.Linear(added_kv_proj_dim, cross_attention_dim)\n",
        "            self.add_v_proj = nn.Linear(added_kv_proj_dim, cross_attention_dim)\n",
        "\n",
        "        self.to_out = nn.ModuleList([])\n",
        "        self.to_out.append(nn.Linear(inner_dim, query_dim))\n",
        "        self.to_out.append(nn.Dropout(dropout))\n",
        "\n",
        "    def reshape_heads_to_batch_dim(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.heads\n",
        "        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
        "        return tensor\n",
        "\n",
        "    def reshape_batch_dim_to_heads(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.heads\n",
        "        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
        "        return tensor\n",
        "\n",
        "    def set_attention_slice(self, slice_size):\n",
        "        if slice_size is not None and slice_size > self.sliceable_head_dim:\n",
        "            raise ValueError(f\"slice_size {slice_size} has to be smaller or equal to {self.sliceable_head_dim}.\")\n",
        "\n",
        "        self._slice_size = slice_size\n",
        "\n",
        "    def forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
        "        batch_size, sequence_length, _ = hidden_states.shape\n",
        "\n",
        "        encoder_hidden_states = encoder_hidden_states\n",
        "\n",
        "        if self.group_norm is not None:\n",
        "            hidden_states = self.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = self.to_q(hidden_states)\n",
        "        dim = query.shape[-1]\n",
        "        query = self.reshape_heads_to_batch_dim(query)\n",
        "\n",
        "        if self.added_kv_proj_dim is not None:\n",
        "            key = self.to_k(hidden_states)\n",
        "            value = self.to_v(hidden_states)\n",
        "            encoder_hidden_states_key_proj = self.add_k_proj(encoder_hidden_states)\n",
        "            encoder_hidden_states_value_proj = self.add_v_proj(encoder_hidden_states)\n",
        "\n",
        "            key = self.reshape_heads_to_batch_dim(key)\n",
        "            value = self.reshape_heads_to_batch_dim(value)\n",
        "            encoder_hidden_states_key_proj = self.reshape_heads_to_batch_dim(encoder_hidden_states_key_proj)\n",
        "            encoder_hidden_states_value_proj = self.reshape_heads_to_batch_dim(encoder_hidden_states_value_proj)\n",
        "\n",
        "            key = torch.concat([encoder_hidden_states_key_proj, key], dim=1)\n",
        "            value = torch.concat([encoder_hidden_states_value_proj, value], dim=1)\n",
        "        else:\n",
        "            encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n",
        "            key = self.to_k(encoder_hidden_states)\n",
        "            value = self.to_v(encoder_hidden_states)\n",
        "\n",
        "            key = self.reshape_heads_to_batch_dim(key)\n",
        "            value = self.reshape_heads_to_batch_dim(value)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.shape[-1] != query.shape[1]:\n",
        "                target_length = query.shape[1]\n",
        "                attention_mask = F.pad(attention_mask, (0, target_length), value=0.0)\n",
        "                attention_mask = attention_mask.repeat_interleave(self.heads, dim=0)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        if self._use_memory_efficient_attention_xformers:\n",
        "            hidden_states = self._memory_efficient_attention_xformers(query, key, value, attention_mask)\n",
        "            # Some versions of xformers return output in fp32, cast it back to the dtype of the input\n",
        "            hidden_states = hidden_states.to(query.dtype)\n",
        "        else:\n",
        "            if self._slice_size is None or query.shape[0] // self._slice_size == 1:\n",
        "                hidden_states = self._attention(query, key, value, attention_mask)\n",
        "            else:\n",
        "                hidden_states = self._sliced_attention(query, key, value, sequence_length, dim, attention_mask)\n",
        "\n",
        "        # linear proj\n",
        "        hidden_states = self.to_out[0](hidden_states)\n",
        "\n",
        "        # dropout\n",
        "        hidden_states = self.to_out[1](hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    def _attention(self, query, key, value, attention_mask=None):\n",
        "        if self.upcast_attention:\n",
        "            query = query.float()\n",
        "            key = key.float()\n",
        "\n",
        "        attention_scores = torch.baddbmm(\n",
        "            torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
        "            query,\n",
        "            key.transpose(-1, -2),\n",
        "            beta=0,\n",
        "            alpha=self.scale,\n",
        "        )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        if self.upcast_softmax:\n",
        "            attention_scores = attention_scores.float()\n",
        "\n",
        "        attention_probs = attention_scores.softmax(dim=-1)\n",
        "\n",
        "        # cast back to the original dtype\n",
        "        attention_probs = attention_probs.to(value.dtype)\n",
        "\n",
        "        # compute attention output\n",
        "        hidden_states = torch.bmm(attention_probs, value)\n",
        "\n",
        "        # reshape hidden_states\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    def _sliced_attention(self, query, key, value, sequence_length, dim, attention_mask):\n",
        "        batch_size_attention = query.shape[0]\n",
        "        hidden_states = torch.zeros(\n",
        "            (batch_size_attention, sequence_length, dim // self.heads), device=query.device, dtype=query.dtype\n",
        "        )\n",
        "        slice_size = self._slice_size if self._slice_size is not None else hidden_states.shape[0]\n",
        "        for i in range(hidden_states.shape[0] // slice_size):\n",
        "            start_idx = i * slice_size\n",
        "            end_idx = (i + 1) * slice_size\n",
        "\n",
        "            query_slice = query[start_idx:end_idx]\n",
        "            key_slice = key[start_idx:end_idx]\n",
        "\n",
        "            if self.upcast_attention:\n",
        "                query_slice = query_slice.float()\n",
        "                key_slice = key_slice.float()\n",
        "\n",
        "            attn_slice = torch.baddbmm(\n",
        "                torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query_slice.dtype, device=query.device),\n",
        "                query_slice,\n",
        "                key_slice.transpose(-1, -2),\n",
        "                beta=0,\n",
        "                alpha=self.scale,\n",
        "            )\n",
        "\n",
        "            if attention_mask is not None:\n",
        "                attn_slice = attn_slice + attention_mask[start_idx:end_idx]\n",
        "\n",
        "            if self.upcast_softmax:\n",
        "                attn_slice = attn_slice.float()\n",
        "\n",
        "            attn_slice = attn_slice.softmax(dim=-1)\n",
        "\n",
        "            # cast back to the original dtype\n",
        "            attn_slice = attn_slice.to(value.dtype)\n",
        "            attn_slice = torch.bmm(attn_slice, value[start_idx:end_idx])\n",
        "\n",
        "            hidden_states[start_idx:end_idx] = attn_slice\n",
        "\n",
        "        # reshape hidden_states\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    def _memory_efficient_attention_xformers(self, query, key, value, attention_mask):\n",
        "        # TODO attention_mask\n",
        "        query = query.contiguous()\n",
        "        key = key.contiguous()\n",
        "        value = value.contiguous()\n",
        "        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "WM3MFfYureF_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example"
      ],
      "metadata": {
        "id": "uwmEbzlDjSbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    r\"\"\"\n",
        "    A cross attention layer.\n",
        "\n",
        "    Parameters:\n",
        "        query_dim (`int`): The number of channels in the query.\n",
        "        cross_attention_dim (`int`, *optional*):\n",
        "            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n",
        "        heads (`int`,  *optional*, defaults to 8): The number of heads to use for multi-head attention.\n",
        "        dim_head (`int`,  *optional*, defaults to 64): The number of channels in each head.\n",
        "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
        "        bias (`bool`, *optional*, defaults to False):\n",
        "            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_dim: int = 2,\n",
        "        heads: int = 8,\n",
        "        dim_head: int = 64,\n",
        "        bias=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=bias)\n",
        "        self.to_k = nn.Linear(query_dim, inner_dim, bias=bias)\n",
        "        self.to_v = nn.Linear(query_dim, inner_dim, bias=bias)\n",
        "\n",
        "        self.to_out = nn.ModuleList([])\n",
        "        self.to_out.append(nn.Linear(inner_dim, query_dim))\n",
        "\n",
        "\n",
        "    def reshape_heads_to_batch_dim(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.heads\n",
        "        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
        "        return tensor\n",
        "\n",
        "    def reshape_batch_dim_to_heads(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.heads\n",
        "        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
        "        batch_size, sequence_length, _ = hidden_states.shape\n",
        "\n",
        "        query = self.to_q(hidden_states)\n",
        "        print(\"after to_q (linear )\", query.shape )\n",
        "        key = self.to_k(hidden_states)\n",
        "        value = self.to_v(hidden_states)\n",
        "\n",
        "        dim = query.shape[-1]\n",
        "        query = self.reshape_heads_to_batch_dim(query)\n",
        "        print(\"after reshape (headstobatch )\", query.shape )\n",
        "        key = self.reshape_heads_to_batch_dim(key)\n",
        "        value = self.reshape_heads_to_batch_dim(value)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        hidden_states = self._attention(query, key, value)\n",
        "\n",
        "        # linear proj\n",
        "        hidden_states = self.to_out[0](hidden_states)\n",
        "        print(\"after to_out (linear )\", hidden_states.shape )\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "    def _attention(self, query, key, value):\n",
        "\n",
        "        attention_scores = torch.baddbmm(\n",
        "            torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
        "            query,\n",
        "            key.transpose(-1, -2),\n",
        "            beta=0,\n",
        "            alpha=1,\n",
        "        )\n",
        "\n",
        "        attention_probs = attention_scores.softmax(dim=-1)\n",
        "        print(\" attention weight shape \", attention_probs.shape)\n",
        "        # cast back to the original dtype\n",
        "        attention_probs = attention_probs.to(value.dtype)\n",
        "\n",
        "        # compute attention output\n",
        "        hidden_states = torch.bmm(attention_probs, value)\n",
        "        print(\" attention before reshapebatchtohead \", hidden_states.shape)\n",
        "\n",
        "        # reshape hidden_states\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        print(\" attention after reshapebatchtohead \", hidden_states.shape)\n",
        "        return hidden_states\n",
        "\n",
        "# Initialize the module\n",
        "cross_attention_layer = CrossAttention(query_dim,heads, dim_head )"
      ],
      "metadata": {
        "id": "e9SLveqcjVts"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "heads = 2\n",
        "seq_len = 6\n",
        "query_dim = 3\n",
        "dim_head=2\n",
        "\n",
        "\n",
        "\n",
        "hidden_states = torch.rand(batch_size, seq_len, query_dim)\n",
        "print(\"input shape:\", hidden_states.shape)\n",
        "\n",
        "\n",
        "output = cross_attention_layer(hidden_states)\n",
        "print(\"output shape:\", output.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "bZXHPsDDlwJ8",
        "outputId": "33ceb646-537d-4844-badd-d471935a83c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([2, 6, 3])\n",
            "after to_q (linear ) torch.Size([2, 6, 4])\n",
            "after reshape (headstobatch ) torch.Size([4, 6, 2])\n",
            " attention weight shape  torch.Size([4, 6, 6])\n",
            " attention before reshapebatchtohead  torch.Size([4, 6, 2])\n",
            " attention after reshapebatchtohead  torch.Size([2, 6, 4])\n",
            "after to_out (linear ) torch.Size([2, 6, 3])\n",
            "output shape: torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "SZuCq7aw0CDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops\n"
      ],
      "metadata": {
        "id": "D4_OKmL3rlsA",
        "outputId": "163239a7-fc86-46c0-fba8-c14d877f4a13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rwbzpXtUV85e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.self_attention"
      ],
      "metadata": {
        "id": "R_4jnLicHQCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1alg"
      ],
      "metadata": {
        "id": "XVp7sdZnF2tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v  ):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "        #print(\"W_query shape:\", self.W_query)\n",
        "        print(\"W_key shape:\", self.W_key, self.W_key.shape)\n",
        "\n",
        "        #print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        print(\"keys \", keys, keys.shape )\n",
        "        print(\"x\", x, x.shape )\n",
        "        #print(\"queries \", queries)\n",
        "        #print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v )\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"head \", dir(self.heads[0]))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "EE1cZUpZ3RtG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for MultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "\n",
        "seq_len = 6\n",
        "batch_size = 2\n",
        "\n",
        "\n",
        "\n",
        "multi_head_attention = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input shape:\", x.shape,x)\n",
        "output = multi_head_attention(x)\n",
        "#print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKmBdqOu33_8",
        "outputId": "f8285168-3bd7-43b3-f44a-0aeecb9e399a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_key shape: Parameter containing:\n",
            "tensor([[2., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 0.]], requires_grad=True) torch.Size([3, 2])\n",
            "W_key shape: Parameter containing:\n",
            "tensor([[2., 1.],\n",
            "        [0., 1.],\n",
            "        [2., 2.]], requires_grad=True) torch.Size([3, 2])\n",
            "head  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper input shape: torch.Size([2, 6, 3]) tensor([[[1., 1., 1.],\n",
            "         [0., 2., 2.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 1., 1.],\n",
            "         [1., 2., 0.],\n",
            "         [2., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 1.],\n",
            "         [2., 2., 1.],\n",
            "         [2., 0., 2.],\n",
            "         [0., 0., 0.],\n",
            "         [2., 1., 1.],\n",
            "         [0., 1., 1.]]])\n",
            "keys  tensor([[[2., 2.],\n",
            "         [0., 2.],\n",
            "         [0., 0.],\n",
            "         [0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 2.]],\n",
            "\n",
            "        [[2., 1.],\n",
            "         [4., 4.],\n",
            "         [4., 2.],\n",
            "         [0., 0.],\n",
            "         [4., 3.],\n",
            "         [0., 1.]]], grad_fn=<UnsafeViewBackward0>) torch.Size([2, 6, 2])\n",
            "x tensor([[[1., 1., 1.],\n",
            "         [0., 2., 2.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 1., 1.],\n",
            "         [1., 2., 0.],\n",
            "         [2., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 1.],\n",
            "         [2., 2., 1.],\n",
            "         [2., 0., 2.],\n",
            "         [0., 0., 0.],\n",
            "         [2., 1., 1.],\n",
            "         [0., 1., 1.]]]) torch.Size([2, 6, 3])\n",
            "keys  tensor([[[4., 4.],\n",
            "         [4., 6.],\n",
            "         [0., 0.],\n",
            "         [2., 3.],\n",
            "         [2., 3.],\n",
            "         [6., 4.]],\n",
            "\n",
            "        [[4., 3.],\n",
            "         [6., 6.],\n",
            "         [8., 6.],\n",
            "         [0., 0.],\n",
            "         [6., 5.],\n",
            "         [2., 3.]]], grad_fn=<UnsafeViewBackward0>) torch.Size([2, 6, 2])\n",
            "x tensor([[[1., 1., 1.],\n",
            "         [0., 2., 2.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 1., 1.],\n",
            "         [1., 2., 0.],\n",
            "         [2., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 1.],\n",
            "         [2., 2., 1.],\n",
            "         [2., 0., 2.],\n",
            "         [0., 0., 0.],\n",
            "         [2., 1., 1.],\n",
            "         [0., 1., 1.]]]) torch.Size([2, 6, 3])\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(A)\n",
        "print(A.shape)\n",
        "B=torch.tensor([[4,5,6],[7,8,9]])\n",
        "print(B)\n",
        "print(B.shape)\n",
        "print(torch.cat((A,B),dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFSHwJqbxMhW",
        "outputId": "0b0a3534-3c33-4b96-c1b7-773021be1bb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[1, 2, 3, 4, 5, 6],\n",
            "        [4, 5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2two methods compare"
      ],
      "metadata": {
        "id": "g8eiLuUBF-uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the selfttention class from Method 1\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v  ):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "        #print(\"W_query shape:\", self.W_query)\n",
        "        #print(\"W_key shape:\", self.W_key, self.W_key.shape)\n",
        "\n",
        "        #print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        #print(\"keys \", keys, keys.shape )\n",
        "        #print(\"x\", x, x.shape )\n",
        "        #print(\"queries \", queries)\n",
        "        #print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v )\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"head \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        print(\"qkv shape:\",  qkv.shape)\n",
        "        assert width % (3 * self.heads) == 0\n",
        "        ch = width // (3 * self.heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        print(\"q\",  q.shape)\n",
        "        print(\"k\", k.shape)\n",
        "        print(\"v\", v.shape )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        print(\"q*scale shape \", (q*scale).shape)\n",
        "        print(\"new shape \", bs * self.heads, ch, length  )\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.heads, ch, length),\n",
        "            (k * scale).view(bs * self.heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 12\n",
        "d_v = d_k_q\n",
        "num_heads = 1\n",
        "seq_len = 6\n",
        "batch_size = 2\n",
        "# Example input\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input shape:\",   x.shape)\n",
        "\n",
        "# Initialize Method 1\n",
        "multihead_attention_1 = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output_1 = multihead_attention_1(x)\n",
        "#print(\"MultiHeadAttentionWrapper output shape:\", output_1)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output_1.shape )\n",
        "print(\"----------------------------------------------\")\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x @ head.W_query for head in multihead_attention_1.heads]\n",
        "keys = [x @ head.W_key for head in multihead_attention_1.heads]\n",
        "values = [x @ head.W_value for head in multihead_attention_1.heads]\n",
        "#print(\"queries before cat \", queries)\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "#print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1).shape )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv.shape  )\n",
        "\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "qkv_attention = QKVAttention(num_heads)\n",
        "output_2 = qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1.shape)\n",
        "print(\"Output from Method 2:\\n\",  output_2.shape)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZzKWxwBd-Y",
        "outputId": "2221123d-eea7-47f6-98f9-9a4f258b08c2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttentionWrapper input shape: torch.Size([2, 6, 4])\n",
            "head  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([2, 6, 12])\n",
            "----------------------------------------------\n",
            "before permute qkv torch.Size([2, 6, 36])\n",
            "after permute qkv torch.Size([2, 36, 6])\n",
            "qkv shape: torch.Size([2, 36, 6])\n",
            "q torch.Size([2, 12, 6])\n",
            "k torch.Size([2, 12, 6])\n",
            "v torch.Size([2, 12, 6])\n",
            "q*scale shape  torch.Size([2, 12, 6])\n",
            "new shape  2 12 6\n",
            "Output from Method 1:\n",
            " torch.Size([2, 12, 6])\n",
            "Output from Method 2:\n",
            " torch.Size([2, 12, 6])\n",
            "Are the outputs close?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3attention_map\n",
        "https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb"
      ],
      "metadata": {
        "id": "om-WM61h0Bfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.CrossAttention"
      ],
      "metadata": {
        "id": "dJRtOrM46kkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1alg"
      ],
      "metadata": {
        "id": "D2JBgzJNHJvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "2lCfxcpB7smg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for CrossMultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "batch_size = 1\n",
        "seq_len = 6\n",
        "\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input1 shape:\", x_1)\n",
        "print(\"MultiHeadAttentionWrapper input2 shape:\", x_2)\n",
        "output = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZcqEqSWIxep",
        "outputId": "9237b7d6-f055-4dbf-e5b8-70d0370d99d1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper input1 shape: tensor([[[2., 2., 0.],\n",
            "         [1., 2., 0.],\n",
            "         [0., 2., 0.],\n",
            "         [2., 0., 0.],\n",
            "         [0., 2., 2.],\n",
            "         [1., 2., 2.]]])\n",
            "MultiHeadAttentionWrapper input2 shape: tensor([[[1., 0., 2.],\n",
            "         [2., 2., 0.],\n",
            "         [0., 1., 1.],\n",
            "         [2., 1., 1.],\n",
            "         [2., 1., 2.],\n",
            "         [0., 0., 1.]]])\n",
            "queries_1 shape: tensor([[[4., 4.],\n",
            "         [2., 4.],\n",
            "         [0., 4.],\n",
            "         [4., 0.],\n",
            "         [0., 4.],\n",
            "         [2., 4.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[5., 1.],\n",
            "         [2., 6.],\n",
            "         [2., 2.],\n",
            "         [4., 4.],\n",
            "         [6., 4.],\n",
            "         [2., 0.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[5., 4.],\n",
            "         [6., 4.],\n",
            "         [4., 1.],\n",
            "         [6., 5.],\n",
            "         [8., 6.],\n",
            "         [2., 1.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "queries_1 shape: tensor([[[2., 4.],\n",
            "         [1., 4.],\n",
            "         [0., 4.],\n",
            "         [2., 0.],\n",
            "         [4., 4.],\n",
            "         [5., 4.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[0., 0.],\n",
            "         [4., 4.],\n",
            "         [2., 2.],\n",
            "         [2., 2.],\n",
            "         [2., 2.],\n",
            "         [0., 0.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[2., 0.],\n",
            "         [0., 2.],\n",
            "         [1., 1.],\n",
            "         [1., 1.],\n",
            "         [2., 1.],\n",
            "         [1., 0.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[7.9861e+00, 5.9896e+00, 8.2556e-04, 1.9994e+00],\n",
            "         [6.9712e+00, 5.0000e+00, 3.3908e-03, 1.9975e+00],\n",
            "         [6.0069e+00, 4.0104e+00, 1.3865e-02, 1.9896e+00],\n",
            "         [7.8264e+00, 5.8853e+00, 2.0848e-01, 1.8385e+00],\n",
            "         [6.0069e+00, 4.0104e+00, 4.8817e-05, 2.0000e+00],\n",
            "         [6.9712e+00, 5.0000e+00, 1.1868e-05, 2.0000e+00]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 two methods compare"
      ],
      "metadata": {
        "id": "BZcr1G8ZKe2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the QKVAttention class from Method 1\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "        print(\"queries_1 shape:\", queries_1)\n",
        "        print(\"keys_2 shape:\", keys_2)\n",
        "        print(\"values_2 shape:\", values_2)\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "    def reshape_heads_to_batch_dim(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.n_heads\n",
        "        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
        "        return tensor\n",
        "\n",
        "    def reshape_batch_dim_to_heads(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.heads\n",
        "        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
        "        return tensor\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class CrossQKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        print(\"q\", q, q.shape)\n",
        "        print(\"k\", k.shape)\n",
        "        print(\"v\", v.shape )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "seq_len = 6\n",
        "batch_size = 2\n",
        "\n",
        "# Example input\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"CrossMultiHeadAttentionWrapper input 1 shape:\", x_1)\n",
        "print(\"CrossMultiHeadAttentionWrapper input 2 shape:\", x_2)\n",
        "\n",
        "# Initialize Method 1\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output_1 = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output_1)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output_1.shape )\n",
        "\n",
        "print(\"-----------------------------------------------------\")\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x_1 @ head.W_query for head in cross_multi_head_attention.heads]\n",
        "keys = [x_2 @ head.W_key for head in cross_multi_head_attention.heads]\n",
        "values = [x_2 @ head.W_value for head in cross_multi_head_attention.heads]\n",
        "print(\"queries before cat \", queries)\n",
        "print(\"keys before cat \", keys)\n",
        "print(\"values before cat \", values)\n",
        "\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1) )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv  )\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "cross_qkv_attention = CrossQKVAttention(num_heads)\n",
        "output_2 = cross_qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1)\n",
        "print(\"Output from Method 2:\\n\", output_2)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oxGKS6nKmOb",
        "outputId": "58612dde-0ff3-42d6-d042-0aebc85bbb44"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossMultiHeadAttentionWrapper input 1 shape: tensor([[[1., 2., 3., 0.],\n",
            "         [1., 2., 2., 0.],\n",
            "         [2., 2., 3., 1.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [3., 1., 3., 0.],\n",
            "         [0., 3., 3., 0.]],\n",
            "\n",
            "        [[1., 3., 3., 3.],\n",
            "         [2., 0., 2., 3.],\n",
            "         [1., 2., 0., 3.],\n",
            "         [3., 2., 2., 3.],\n",
            "         [1., 3., 3., 0.],\n",
            "         [1., 2., 0., 1.]]])\n",
            "CrossMultiHeadAttentionWrapper input 2 shape: tensor([[[0., 3., 1., 3.],\n",
            "         [3., 2., 0., 2.],\n",
            "         [2., 2., 0., 0.],\n",
            "         [1., 3., 0., 2.],\n",
            "         [3., 0., 1., 3.],\n",
            "         [0., 1., 1., 2.]],\n",
            "\n",
            "        [[3., 0., 1., 2.],\n",
            "         [0., 0., 3., 0.],\n",
            "         [2., 2., 2., 0.],\n",
            "         [3., 3., 2., 1.],\n",
            "         [2., 3., 3., 3.],\n",
            "         [1., 1., 3., 1.]]])\n",
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "queries_1 shape: tensor([[[11.,  8.],\n",
            "         [ 9.,  7.],\n",
            "         [15., 11.],\n",
            "         [15., 11.],\n",
            "         [11.,  8.],\n",
            "         [12.,  9.]],\n",
            "\n",
            "        [[22., 16.],\n",
            "         [15., 10.],\n",
            "         [14., 11.],\n",
            "         [20., 15.],\n",
            "         [13., 10.],\n",
            "         [ 8.,  7.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[17.,  9.],\n",
            "         [16.,  7.],\n",
            "         [ 8.,  2.],\n",
            "         [14.,  5.],\n",
            "         [17., 12.],\n",
            "         [10.,  7.]],\n",
            "\n",
            "        [[14., 10.],\n",
            "         [ 6.,  9.],\n",
            "         [12.,  8.],\n",
            "         [19., 11.],\n",
            "         [25., 17.],\n",
            "         [13., 12.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[12.,  7.],\n",
            "         [17.,  7.],\n",
            "         [ 8.,  6.],\n",
            "         [12.,  7.],\n",
            "         [18.,  4.],\n",
            "         [ 7.,  3.]],\n",
            "\n",
            "        [[15.,  4.],\n",
            "         [ 0.,  3.],\n",
            "         [ 8.,  8.],\n",
            "         [15., 11.],\n",
            "         [18., 11.],\n",
            "         [ 7.,  6.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "queries_1 shape: tensor([[[17.,  1.],\n",
            "         [14.,  1.],\n",
            "         [19.,  3.],\n",
            "         [14.,  3.],\n",
            "         [18.,  3.],\n",
            "         [18.,  0.]],\n",
            "\n",
            "        [[20.,  4.],\n",
            "         [10.,  5.],\n",
            "         [ 8.,  4.],\n",
            "         [18.,  6.],\n",
            "         [20.,  1.],\n",
            "         [ 8.,  2.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[ 9., 18.],\n",
            "         [ 7., 12.],\n",
            "         [ 2.,  6.],\n",
            "         [ 5., 15.],\n",
            "         [12.,  9.],\n",
            "         [ 7.,  9.]],\n",
            "\n",
            "        [[10.,  6.],\n",
            "         [ 9.,  0.],\n",
            "         [ 8.,  6.],\n",
            "         [11., 12.],\n",
            "         [17., 18.],\n",
            "         [12.,  6.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[ 9., 12.],\n",
            "         [12.,  8.],\n",
            "         [ 4.,  6.],\n",
            "         [ 8., 11.],\n",
            "         [15.,  3.],\n",
            "         [ 6.,  5.]],\n",
            "\n",
            "        [[12.,  2.],\n",
            "         [ 0.,  0.],\n",
            "         [ 4.,  6.],\n",
            "         [ 9., 10.],\n",
            "         [13., 12.],\n",
            "         [ 5.,  4.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[18.0000,  4.0000, 15.0000,  3.0000],\n",
            "         [18.0000,  4.0000, 15.0000,  3.0000],\n",
            "         [18.0000,  4.0000, 15.0000,  3.0000],\n",
            "         [18.0000,  4.0000, 14.9999,  3.0002],\n",
            "         [18.0000,  4.0000, 15.0000,  3.0000],\n",
            "         [18.0000,  4.0000, 15.0000,  3.0000]],\n",
            "\n",
            "        [[18.0000, 11.0000, 13.0000, 12.0000],\n",
            "         [18.0000, 11.0000, 13.0000, 12.0000],\n",
            "         [18.0000, 11.0000, 13.0000, 12.0000],\n",
            "         [18.0000, 11.0000, 13.0000, 12.0000],\n",
            "         [18.0000, 11.0000, 13.0000, 12.0000],\n",
            "         [18.0000, 11.0000, 13.0000, 12.0000]]], grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([2, 6, 4])\n",
            "-----------------------------------------------------\n",
            "queries before cat  [tensor([[[11.,  8.],\n",
            "         [ 9.,  7.],\n",
            "         [15., 11.],\n",
            "         [15., 11.],\n",
            "         [11.,  8.],\n",
            "         [12.,  9.]],\n",
            "\n",
            "        [[22., 16.],\n",
            "         [15., 10.],\n",
            "         [14., 11.],\n",
            "         [20., 15.],\n",
            "         [13., 10.],\n",
            "         [ 8.,  7.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[17.,  1.],\n",
            "         [14.,  1.],\n",
            "         [19.,  3.],\n",
            "         [14.,  3.],\n",
            "         [18.,  3.],\n",
            "         [18.,  0.]],\n",
            "\n",
            "        [[20.,  4.],\n",
            "         [10.,  5.],\n",
            "         [ 8.,  4.],\n",
            "         [18.,  6.],\n",
            "         [20.,  1.],\n",
            "         [ 8.,  2.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "keys before cat  [tensor([[[17.,  9.],\n",
            "         [16.,  7.],\n",
            "         [ 8.,  2.],\n",
            "         [14.,  5.],\n",
            "         [17., 12.],\n",
            "         [10.,  7.]],\n",
            "\n",
            "        [[14., 10.],\n",
            "         [ 6.,  9.],\n",
            "         [12.,  8.],\n",
            "         [19., 11.],\n",
            "         [25., 17.],\n",
            "         [13., 12.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[ 9., 18.],\n",
            "         [ 7., 12.],\n",
            "         [ 2.,  6.],\n",
            "         [ 5., 15.],\n",
            "         [12.,  9.],\n",
            "         [ 7.,  9.]],\n",
            "\n",
            "        [[10.,  6.],\n",
            "         [ 9.,  0.],\n",
            "         [ 8.,  6.],\n",
            "         [11., 12.],\n",
            "         [17., 18.],\n",
            "         [12.,  6.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "values before cat  [tensor([[[12.,  7.],\n",
            "         [17.,  7.],\n",
            "         [ 8.,  6.],\n",
            "         [12.,  7.],\n",
            "         [18.,  4.],\n",
            "         [ 7.,  3.]],\n",
            "\n",
            "        [[15.,  4.],\n",
            "         [ 0.,  3.],\n",
            "         [ 8.,  8.],\n",
            "         [15., 11.],\n",
            "         [18., 11.],\n",
            "         [ 7.,  6.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[ 9., 12.],\n",
            "         [12.,  8.],\n",
            "         [ 4.,  6.],\n",
            "         [ 8., 11.],\n",
            "         [15.,  3.],\n",
            "         [ 6.,  5.]],\n",
            "\n",
            "        [[12.,  2.],\n",
            "         [ 0.,  0.],\n",
            "         [ 4.,  6.],\n",
            "         [ 9., 10.],\n",
            "         [13., 12.],\n",
            "         [ 5.,  4.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "queries after cat tensor([[[11.,  8., 17.,  1.],\n",
            "         [ 9.,  7., 14.,  1.],\n",
            "         [15., 11., 19.,  3.],\n",
            "         [15., 11., 14.,  3.],\n",
            "         [11.,  8., 18.,  3.],\n",
            "         [12.,  9., 18.,  0.]],\n",
            "\n",
            "        [[22., 16., 20.,  4.],\n",
            "         [15., 10., 10.,  5.],\n",
            "         [14., 11.,  8.,  4.],\n",
            "         [20., 15., 18.,  6.],\n",
            "         [13., 10., 20.,  1.],\n",
            "         [ 8.,  7.,  8.,  2.]]], grad_fn=<CatBackward0>)\n",
            "before permute qkv tensor([[[11.,  8., 17.,  1., 17.,  9.,  9., 18., 12.,  7.,  9., 12.],\n",
            "         [ 9.,  7., 14.,  1., 16.,  7.,  7., 12., 17.,  7., 12.,  8.],\n",
            "         [15., 11., 19.,  3.,  8.,  2.,  2.,  6.,  8.,  6.,  4.,  6.],\n",
            "         [15., 11., 14.,  3., 14.,  5.,  5., 15., 12.,  7.,  8., 11.],\n",
            "         [11.,  8., 18.,  3., 17., 12., 12.,  9., 18.,  4., 15.,  3.],\n",
            "         [12.,  9., 18.,  0., 10.,  7.,  7.,  9.,  7.,  3.,  6.,  5.]],\n",
            "\n",
            "        [[22., 16., 20.,  4., 14., 10., 10.,  6., 15.,  4., 12.,  2.],\n",
            "         [15., 10., 10.,  5.,  6.,  9.,  9.,  0.,  0.,  3.,  0.,  0.],\n",
            "         [14., 11.,  8.,  4., 12.,  8.,  8.,  6.,  8.,  8.,  4.,  6.],\n",
            "         [20., 15., 18.,  6., 19., 11., 11., 12., 15., 11.,  9., 10.],\n",
            "         [13., 10., 20.,  1., 25., 17., 17., 18., 18., 11., 13., 12.],\n",
            "         [ 8.,  7.,  8.,  2., 13., 12., 12.,  6.,  7.,  6.,  5.,  4.]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "after permute qkv tensor([[[11.,  9., 15., 15., 11., 12.],\n",
            "         [ 8.,  7., 11., 11.,  8.,  9.],\n",
            "         [17., 14., 19., 14., 18., 18.],\n",
            "         [ 1.,  1.,  3.,  3.,  3.,  0.],\n",
            "         [17., 16.,  8., 14., 17., 10.],\n",
            "         [ 9.,  7.,  2.,  5., 12.,  7.],\n",
            "         [ 9.,  7.,  2.,  5., 12.,  7.],\n",
            "         [18., 12.,  6., 15.,  9.,  9.],\n",
            "         [12., 17.,  8., 12., 18.,  7.],\n",
            "         [ 7.,  7.,  6.,  7.,  4.,  3.],\n",
            "         [ 9., 12.,  4.,  8., 15.,  6.],\n",
            "         [12.,  8.,  6., 11.,  3.,  5.]],\n",
            "\n",
            "        [[22., 15., 14., 20., 13.,  8.],\n",
            "         [16., 10., 11., 15., 10.,  7.],\n",
            "         [20., 10.,  8., 18., 20.,  8.],\n",
            "         [ 4.,  5.,  4.,  6.,  1.,  2.],\n",
            "         [14.,  6., 12., 19., 25., 13.],\n",
            "         [10.,  9.,  8., 11., 17., 12.],\n",
            "         [10.,  9.,  8., 11., 17., 12.],\n",
            "         [ 6.,  0.,  6., 12., 18.,  6.],\n",
            "         [15.,  0.,  8., 15., 18.,  7.],\n",
            "         [ 4.,  3.,  8., 11., 11.,  6.],\n",
            "         [12.,  0.,  4.,  9., 13.,  5.],\n",
            "         [ 2.,  0.,  6., 10., 12.,  4.]]], grad_fn=<PermuteBackward0>)\n",
            "q tensor([[[11.,  9., 15., 15., 11., 12.],\n",
            "         [ 8.,  7., 11., 11.,  8.,  9.],\n",
            "         [17., 14., 19., 14., 18., 18.],\n",
            "         [ 1.,  1.,  3.,  3.,  3.,  0.]],\n",
            "\n",
            "        [[22., 15., 14., 20., 13.,  8.],\n",
            "         [16., 10., 11., 15., 10.,  7.],\n",
            "         [20., 10.,  8., 18., 20.,  8.],\n",
            "         [ 4.,  5.,  4.,  6.,  1.,  2.]]], grad_fn=<SplitBackward0>) torch.Size([2, 4, 6])\n",
            "k torch.Size([2, 4, 6])\n",
            "v torch.Size([2, 4, 6])\n",
            "Output from Method 1:\n",
            " tensor([[[18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000],\n",
            "         [ 4.0000,  4.0000,  4.0000,  4.0000,  4.0000,  4.0000],\n",
            "         [15.0000, 15.0000, 15.0000, 14.9999, 15.0000, 15.0000],\n",
            "         [ 3.0000,  3.0000,  3.0000,  3.0002,  3.0000,  3.0000]],\n",
            "\n",
            "        [[18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000],\n",
            "         [11.0000, 11.0000, 11.0000, 11.0000, 11.0000, 11.0000],\n",
            "         [13.0000, 13.0000, 13.0000, 13.0000, 13.0000, 13.0000],\n",
            "         [12.0000, 12.0000, 12.0000, 12.0000, 12.0000, 12.0000]]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "Output from Method 2:\n",
            " tensor([[[18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000],\n",
            "         [ 4.0000,  4.0000,  4.0000,  4.0000,  4.0000,  4.0000],\n",
            "         [15.0000, 15.0000, 15.0000, 14.9999, 15.0000, 15.0000],\n",
            "         [ 3.0000,  3.0000,  3.0000,  3.0002,  3.0000,  3.0000]],\n",
            "\n",
            "        [[18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000],\n",
            "         [11.0000, 11.0000, 11.0000, 11.0000, 11.0000, 11.0000],\n",
            "         [13.0000, 13.0000, 13.0000, 13.0000, 13.0000, 13.0000],\n",
            "         [12.0000, 12.0000, 12.0000, 12.0000, 12.0000, 12.0000]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Are the outputs close?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test AnimateDiff versatileAttention"
      ],
      "metadata": {
        "id": "a0mMhLczqwYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d=2\n",
        "batch_size=5\n",
        "seq_len=6\n",
        "tensor=torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(tensor.shape)\n",
        "\n",
        "output1=rearrange(tensor, \"(b f) d c -> (b d) f c\", f=5)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "EvYcBhg4qvQr",
        "outputId": "0b0beb8f-537e-41df-bd39-eceffddbc7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 6, 2])\n",
            "torch.Size([6, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        dropout = 0.,\n",
        "        max_len = 24\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "pos_encoder = PositionalEncoding(\n",
        "            d_model=2,#kwargs[\"query_dim\"],\n",
        "            dropout=0.,\n",
        "            max_len=24,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "Zxb2FF1Jt3B3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output2=pos_encoder(output1)\n",
        "print(output2.shape)"
      ],
      "metadata": {
        "id": "ADJi811Bulxv",
        "outputId": "cfcdd8cf-1dec-42c1-a22b-271c41c2acde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test JoliGEN UNet_QKVattention"
      ],
      "metadata": {
        "id": "DLOH-Lgr9xXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channels=512\n",
        "d=3\n",
        "qkv = nn.Conv1d(channels, channels * 3, 1)\n",
        "input1=torch.randint(low=0,high=d,size=(5, 512,16*16 )).float()\n",
        "print(input1.shape)\n",
        "op1=qkv(input1)\n",
        "print(op1.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "Mt-A4kF-92zh",
        "outputId": "30747d7e-7525-4b12-e846-c0fab21225fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 512, 256])\n",
            "torch.Size([5, 1536, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the zero_module function\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# Define a neural network module\n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(MyNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = zero_module(nn.Conv1d(channels, channels, kernel_size=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "channels = 16\n",
        "net = MyNetwork(channels=channels)\n",
        "\n",
        "# Print the network to verify the layers\n",
        "print(net)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "input_tensor = torch.randn(1, channels, 10)  # (batch_size, channels, length)\n",
        "\n",
        "# Forward pass\n",
        "output_tensor = net(input_tensor)\n",
        "\n",
        "# Print the output\n",
        "print(output_tensor)\n"
      ],
      "metadata": {
        "id": "DGkBOrF2dFA_",
        "outputId": "ab4c352c-b049-45db-c5fa-4b1e2ef29b4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNetwork(\n",
            "  (conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(16, 16, kernel_size=(1,), stride=(1,))\n",
            ")\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
            "       grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "class QKVAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which performs QKV attention and splits in a different order.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"\n",
        "        Apply QKV attention.\n",
        "        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\n",
        "        \"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        print(\"q\", q.shape)\n",
        "        print(\"k\", k.shape)\n",
        "        print(\"v\", v.shape )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        print(\"weight\", weight.shape)\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        print(\"a shape \", a.shape)\n",
        "        print( a.reshape(bs, -1, length).shape )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "qkv = torch.randint(low=0, high=10, size=(1, 192, 256)).float()\n",
        "n_heads = 2\n",
        "\n",
        "att_module = QKVAttention(n_heads)\n",
        "output = att_module(qkv)\n",
        "print(\"Output shape:\", output.shape)  #"
      ],
      "metadata": {
        "id": "DCBCcWrln_M6",
        "outputId": "5a680ba8-ffbb-401f-bb46-81401393f571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q torch.Size([1, 64, 256])\n",
            "k torch.Size([1, 64, 256])\n",
            "v torch.Size([1, 64, 256])\n",
            "weight torch.Size([2, 256, 256])\n",
            "a shape  torch.Size([2, 32, 256])\n",
            "torch.Size([1, 64, 256])\n",
            "Output shape: torch.Size([1, 64, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SuIqjcRapEGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}