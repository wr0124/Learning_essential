{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9j6gb6QeTrPgUphzo2Y3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wr0124/Learning_essential/blob/main/CorssAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#source\n",
        "\n",
        "\"self_attention\"\n",
        "old: https://github.com/huggingface/diffusers_all/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "new: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py#L41\n",
        "\n",
        "https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "branch: https://github.com/huggingface/diffusers/blob/4125756e88e82370c197fecf28e9f0b4d7eee6c3/src/diffusers/models/cross_attention.py\n",
        "\n",
        "https://github.com/openai/consistency_models/blob/e32b69ee436d518377db86fb2127a3972d0d8716/cm/unet.py#L478\n",
        "\n",
        "https://github.com/openai/guided-diffusion/blob/22e0df8183507e13a7813f8d38d51b072ca1e67c/guided_diffusion/unet.py#L361\n",
        "\n",
        "\n",
        "other:\n",
        "show attention: https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_ldm.ipynb\n",
        "\n",
        "\"cross_attention\"\n",
        "\n",
        "https://github.com/openai/point-e/blob/fc8a607c08a3ea804cc82bf1ef8628f88a3a5d2f/point_e/models/perceiver.py#L11\n",
        "\n",
        "https://github.com/openai/shap-e/blob/50131012ee11c9d2617f3886c10f000d3c7a3b43/shap_e/models/generation/perceiver.py#L13\n",
        "\n",
        "https://github.com/CompVis/latent-diffusion/blob/a506df5756472e2ebaf9078affdde2c4f1502cd4/ldm/modules/attention.py#L152\n",
        "\n",
        "https://blog.csdn.net/weixin_54338498/article/details/135051918\n",
        "\n",
        "\"attention_map\"\n",
        "https://github.com/google/prompt-to-prompt/tree/main\n"
      ],
      "metadata": {
        "id": "BIzPJV4YjsGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "SZuCq7aw0CDr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rwbzpXtUV85e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.self_attention"
      ],
      "metadata": {
        "id": "R_4jnLicHQCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1alg"
      ],
      "metadata": {
        "id": "XVp7sdZnF2tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "        #print(\"W_query shape:\", self.W_query)\n",
        "        #print(\"W_key shape:\", self.W_key)\n",
        "        #print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        #print(\"keys \", keys)\n",
        "        #print(\"queries \", queries)\n",
        "        #print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "EE1cZUpZ3RtG"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for MultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "batch_size = 1\n",
        "seq_len = 6\n",
        "\n",
        "multi_head_attention = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, sequence_length, d)).float()\n",
        "#print(\"MultiHeadAttentionWrapper input shape:\", x)\n",
        "output = multi_head_attention(x)\n",
        "#print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKmBdqOu33_8",
        "outputId": "767fde1d-ca82-47ba-8214-9c522fdce4c6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(A)\n",
        "print(A.shape)\n",
        "B=torch.tensor([[4,5,6],[7,8,9]])\n",
        "print(B)\n",
        "print(B.shape)\n",
        "print(torch.cat((A,B),dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFSHwJqbxMhW",
        "outputId": "644fad90-82cf-4356-fe06-477eb7e890e9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[1, 2, 3, 4, 5, 6],\n",
            "        [4, 5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2two methods compare"
      ],
      "metadata": {
        "id": "g8eiLuUBF-uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the selfttention class from Method 1\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "        #print(\"W_query shape:\", self.W_query)\n",
        "        #print(\"W_key shape:\", self.W_key)\n",
        "        #print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        print(\"queries \", queries)\n",
        "        print(\"keys \", keys)\n",
        "        print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        #print(\"heads \", self.heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        print(\"q\", q, q.shape)\n",
        "        print(\"k\", k.shape)\n",
        "        print(\"v\", v.shape )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 1\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# Example input\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input shape:\", x)\n",
        "\n",
        "# Initialize Method 1\n",
        "multihead_attention_1 = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output_1 = multihead_attention_1(x)\n",
        "\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x @ head.W_query for head in multihead_attention_1.heads]\n",
        "keys = [x @ head.W_key for head in multihead_attention_1.heads]\n",
        "values = [x @ head.W_value for head in multihead_attention_1.heads]\n",
        "print(\"queries before cat \", queries)\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1) )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv  )\n",
        "\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "qkv_attention = QKVAttention(num_heads)\n",
        "output_2 = qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1, output_1.shape)\n",
        "print(\"Output from Method 2:\\n\", output_2, output_2.shape)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZzKWxwBd-Y",
        "outputId": "bbd5727a-7915-4433-f333-024fba81e183"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttentionWrapper input shape: tensor([[[1., 2., 1., 2.],\n",
            "         [2., 2., 3., 1.],\n",
            "         [2., 0., 3., 3.],\n",
            "         [2., 2., 2., 1.],\n",
            "         [1., 0., 2., 2.],\n",
            "         [2., 0., 1., 0.]]])\n",
            "queries  tensor([[[14., 13.],\n",
            "         [19., 17.],\n",
            "         [17., 21.],\n",
            "         [16., 15.],\n",
            "         [11., 13.],\n",
            "         [ 5.,  8.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys  tensor([[[16., 11.],\n",
            "         [20.,  9.],\n",
            "         [20., 11.],\n",
            "         [17.,  9.],\n",
            "         [13.,  7.],\n",
            "         [ 5.,  2.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values  tensor([[[ 6., 15.],\n",
            "         [ 5., 18.],\n",
            "         [ 3., 18.],\n",
            "         [ 5., 15.],\n",
            "         [ 2., 12.],\n",
            "         [ 0.,  3.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "queries before cat  [tensor([[[14., 13.],\n",
            "         [19., 17.],\n",
            "         [17., 21.],\n",
            "         [16., 15.],\n",
            "         [11., 13.],\n",
            "         [ 5.,  8.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "queries after cat tensor([[[14., 13.],\n",
            "         [19., 17.],\n",
            "         [17., 21.],\n",
            "         [16., 15.],\n",
            "         [11., 13.],\n",
            "         [ 5.,  8.]]], grad_fn=<CatBackward0>)\n",
            "before permute qkv tensor([[[14., 13., 16., 11.,  6., 15.],\n",
            "         [19., 17., 20.,  9.,  5., 18.],\n",
            "         [17., 21., 20., 11.,  3., 18.],\n",
            "         [16., 15., 17.,  9.,  5., 15.],\n",
            "         [11., 13., 13.,  7.,  2., 12.],\n",
            "         [ 5.,  8.,  5.,  2.,  0.,  3.]]], grad_fn=<CatBackward0>)\n",
            "after permute qkv tensor([[[14., 19., 17., 16., 11.,  5.],\n",
            "         [13., 17., 21., 15., 13.,  8.],\n",
            "         [16., 20., 20., 17., 13.,  5.],\n",
            "         [11.,  9., 11.,  9.,  7.,  2.],\n",
            "         [ 6.,  5.,  3.,  5.,  2.,  0.],\n",
            "         [15., 18., 18., 15., 12.,  3.]]], grad_fn=<PermuteBackward0>)\n",
            "q tensor([[[14., 19., 17., 16., 11.,  5.],\n",
            "         [13., 17., 21., 15., 13.,  8.]]], grad_fn=<SplitBackward0>) torch.Size([1, 2, 6])\n",
            "k torch.Size([1, 2, 6])\n",
            "v torch.Size([1, 2, 6])\n",
            "Output from Method 1:\n",
            " tensor([[[ 3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000],\n",
            "         [18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000]]],\n",
            "       grad_fn=<PermuteBackward0>) torch.Size([1, 2, 6])\n",
            "Output from Method 2:\n",
            " tensor([[[ 3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000],\n",
            "         [18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000]]],\n",
            "       grad_fn=<ReshapeAliasBackward0>) torch.Size([1, 2, 6])\n",
            "Are the outputs close?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3attention_map\n",
        "https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb"
      ],
      "metadata": {
        "id": "om-WM61h0Bfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.CrossAttention"
      ],
      "metadata": {
        "id": "dJRtOrM46kkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1alg"
      ],
      "metadata": {
        "id": "D2JBgzJNHJvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "2lCfxcpB7smg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for CrossMultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "batch_size = 1\n",
        "seq_len = 6\n",
        "\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input1 shape:\", x_1)\n",
        "print(\"MultiHeadAttentionWrapper input2 shape:\", x_2)\n",
        "output = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZcqEqSWIxep",
        "outputId": "31584e3c-f3e9-489c-c19d-222d908a8ac3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper input1 shape: tensor([[[0., 1., 1.],\n",
            "         [1., 1., 2.],\n",
            "         [2., 0., 2.],\n",
            "         [0., 2., 1.],\n",
            "         [2., 1., 1.],\n",
            "         [2., 1., 0.]]])\n",
            "MultiHeadAttentionWrapper input2 shape: tensor([[[1., 1., 2.],\n",
            "         [1., 1., 1.],\n",
            "         [0., 2., 0.],\n",
            "         [0., 1., 1.],\n",
            "         [0., 2., 2.],\n",
            "         [0., 0., 2.]]])\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[3.3459, 7.3445, 4.0000, 4.0000],\n",
            "         [3.9441, 7.9441, 4.0000, 4.0000],\n",
            "         [3.9861, 7.9861, 4.0000, 4.0000],\n",
            "         [3.3457, 7.3454, 4.0000, 4.0000],\n",
            "         [3.9441, 7.9441, 4.0000, 4.0000],\n",
            "         [3.7887, 7.7870, 3.9999, 3.9999]]], grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 two methods compare"
      ],
      "metadata": {
        "id": "BZcr1G8ZKe2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the QKVAttention class from Method 1\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "        print(\"queries_1 shape:\", queries_1)\n",
        "        print(\"keys_2 shape:\", keys_2)\n",
        "        print(\"values_2 shape:\", values_2)\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class CrossQKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        print(\"q\", q, q.shape)\n",
        "        print(\"k\", k.shape)\n",
        "        print(\"v\", v.shape )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# Example input\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"CrossMultiHeadAttentionWrapper input 1 shape:\", x_1)\n",
        "print(\"CrossMultiHeadAttentionWrapper input 2 shape:\", x_2)\n",
        "\n",
        "# Initialize Method 1\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output_1 = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "\n",
        "print(\"-----------------------------------------------------\")\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x_1 @ head.W_query for head in cross_multi_head_attention.heads]\n",
        "keys = [x_2 @ head.W_key for head in cross_multi_head_attention.heads]\n",
        "values = [x_2 @ head.W_value for head in cross_multi_head_attention.heads]\n",
        "print(\"queries before cat \", queries)\n",
        "print(\"keys before cat \", keys)\n",
        "print(\"values before cat \", values)\n",
        "\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1) )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv  )\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "cross_qkv_attention = CrossQKVAttention(num_heads)\n",
        "output_2 = cross_qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1)\n",
        "print(\"Output from Method 2:\\n\", output_2)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oxGKS6nKmOb",
        "outputId": "8c9a7749-084b-4a32-de7c-7000087efc93"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossMultiHeadAttentionWrapper input 1 shape: tensor([[[2., 1., 2., 2.],\n",
            "         [1., 2., 0., 2.],\n",
            "         [1., 2., 3., 1.],\n",
            "         [1., 2., 1., 1.],\n",
            "         [2., 1., 0., 1.],\n",
            "         [0., 2., 2., 0.]]])\n",
            "CrossMultiHeadAttentionWrapper input 2 shape: tensor([[[2., 1., 0., 3.],\n",
            "         [2., 1., 2., 3.],\n",
            "         [1., 1., 1., 0.],\n",
            "         [2., 1., 3., 0.],\n",
            "         [3., 2., 1., 2.],\n",
            "         [1., 3., 1., 3.]]])\n",
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "queries_1 shape: tensor([[[10., 15.],\n",
            "         [10.,  9.],\n",
            "         [10., 15.],\n",
            "         [ 8.,  9.],\n",
            "         [ 5.,  6.],\n",
            "         [ 6.,  8.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[12., 11.],\n",
            "         [18., 15.],\n",
            "         [ 5.,  3.],\n",
            "         [12.,  8.],\n",
            "         [14., 11.],\n",
            "         [16., 12.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[ 5., 11.],\n",
            "         [11., 13.],\n",
            "         [ 6.,  5.],\n",
            "         [14.,  8.],\n",
            "         [11., 14.],\n",
            "         [ 8., 17.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "queries_1 shape: tensor([[[1., 7.],\n",
            "         [2., 5.],\n",
            "         [2., 7.],\n",
            "         [2., 5.],\n",
            "         [1., 4.],\n",
            "         [2., 4.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[10., 11.],\n",
            "         [14., 13.],\n",
            "         [ 7.,  3.],\n",
            "         [13.,  5.],\n",
            "         [16., 11.],\n",
            "         [16., 16.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[ 8., 12.],\n",
            "         [12., 16.],\n",
            "         [ 7.,  5.],\n",
            "         [14.,  9.],\n",
            "         [15., 14.],\n",
            "         [11., 20.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[10.0000, 21.0000],\n",
            "         [10.0000, 21.0000],\n",
            "         [ 9.9998, 20.9992],\n",
            "         [10.0000, 21.0000],\n",
            "         [10.0000, 21.0000],\n",
            "         [10.0000, 21.0000]]], grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 2])\n",
            "-----------------------------------------------------\n",
            "queries before cat  [tensor([[[10., 15.],\n",
            "         [10.,  9.],\n",
            "         [10., 15.],\n",
            "         [ 8.,  9.],\n",
            "         [ 5.,  6.],\n",
            "         [ 6.,  8.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[1., 7.],\n",
            "         [2., 5.],\n",
            "         [2., 7.],\n",
            "         [2., 5.],\n",
            "         [1., 4.],\n",
            "         [2., 4.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "keys before cat  [tensor([[[12., 11.],\n",
            "         [18., 15.],\n",
            "         [ 5.,  3.],\n",
            "         [12.,  8.],\n",
            "         [14., 11.],\n",
            "         [16., 12.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[10., 11.],\n",
            "         [14., 13.],\n",
            "         [ 7.,  3.],\n",
            "         [13.,  5.],\n",
            "         [16., 11.],\n",
            "         [16., 16.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "values before cat  [tensor([[[ 5., 11.],\n",
            "         [11., 13.],\n",
            "         [ 6.,  5.],\n",
            "         [14.,  8.],\n",
            "         [11., 14.],\n",
            "         [ 8., 17.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[ 8., 12.],\n",
            "         [12., 16.],\n",
            "         [ 7.,  5.],\n",
            "         [14.,  9.],\n",
            "         [15., 14.],\n",
            "         [11., 20.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "queries after cat tensor([[[10., 15.,  1.,  7.],\n",
            "         [10.,  9.,  2.,  5.],\n",
            "         [10., 15.,  2.,  7.],\n",
            "         [ 8.,  9.,  2.,  5.],\n",
            "         [ 5.,  6.,  1.,  4.],\n",
            "         [ 6.,  8.,  2.,  4.]]], grad_fn=<CatBackward0>)\n",
            "before permute qkv tensor([[[10., 15.,  1.,  7., 12., 11., 10., 11.,  5., 11.,  8., 12.],\n",
            "         [10.,  9.,  2.,  5., 18., 15., 14., 13., 11., 13., 12., 16.],\n",
            "         [10., 15.,  2.,  7.,  5.,  3.,  7.,  3.,  6.,  5.,  7.,  5.],\n",
            "         [ 8.,  9.,  2.,  5., 12.,  8., 13.,  5., 14.,  8., 14.,  9.],\n",
            "         [ 5.,  6.,  1.,  4., 14., 11., 16., 11., 11., 14., 15., 14.],\n",
            "         [ 6.,  8.,  2.,  4., 16., 12., 16., 16.,  8., 17., 11., 20.]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "after permute qkv tensor([[[10., 10., 10.,  8.,  5.,  6.],\n",
            "         [15.,  9., 15.,  9.,  6.,  8.],\n",
            "         [ 1.,  2.,  2.,  2.,  1.,  2.],\n",
            "         [ 7.,  5.,  7.,  5.,  4.,  4.],\n",
            "         [12., 18.,  5., 12., 14., 16.],\n",
            "         [11., 15.,  3.,  8., 11., 12.],\n",
            "         [10., 14.,  7., 13., 16., 16.],\n",
            "         [11., 13.,  3.,  5., 11., 16.],\n",
            "         [ 5., 11.,  6., 14., 11.,  8.],\n",
            "         [11., 13.,  5.,  8., 14., 17.],\n",
            "         [ 8., 12.,  7., 14., 15., 11.],\n",
            "         [12., 16.,  5.,  9., 14., 20.]]], grad_fn=<PermuteBackward0>)\n",
            "q tensor([[[10., 10., 10.,  8.,  5.,  6.],\n",
            "         [15.,  9., 15.,  9.,  6.,  8.],\n",
            "         [ 1.,  2.,  2.,  2.,  1.,  2.],\n",
            "         [ 7.,  5.,  7.,  5.,  4.,  4.]]], grad_fn=<SplitBackward0>) torch.Size([1, 4, 6])\n",
            "k torch.Size([1, 4, 6])\n",
            "v torch.Size([1, 4, 6])\n",
            "Output from Method 1:\n",
            " tensor([[[11.0000, 11.0000, 11.0000, 11.0000, 11.0000, 11.0000],\n",
            "         [13.0000, 13.0000, 13.0000, 13.0000, 13.0000, 13.0000],\n",
            "         [11.0000, 11.0000, 11.0000, 11.0000, 11.0001, 11.0000],\n",
            "         [20.0000, 20.0000, 20.0000, 20.0000, 19.9998, 19.9999]]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "Output from Method 2:\n",
            " tensor([[[11.0000, 11.0000, 11.0000, 11.0000, 11.0000, 11.0000],\n",
            "         [13.0000, 13.0000, 13.0000, 13.0000, 13.0000, 13.0000],\n",
            "         [11.0000, 11.0000, 11.0000, 11.0000, 11.0001, 11.0000],\n",
            "         [20.0000, 20.0000, 20.0000, 20.0000, 19.9998, 19.9999]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Are the outputs close?  True\n"
          ]
        }
      ]
    }
  ]
}