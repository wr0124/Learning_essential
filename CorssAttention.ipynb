{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAvueUNOZ763vHrc/xSqdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wr0124/Learning_essential/blob/main/CorssAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#source\n",
        "\n",
        "\"self_attention\"\n",
        "old: https://github.com/huggingface/diffusers_all/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "new: https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py#L41\n",
        "\n",
        "https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py\n",
        "\n",
        "branch: https://github.com/huggingface/diffusers/blob/4125756e88e82370c197fecf28e9f0b4d7eee6c3/src/diffusers/models/cross_attention.py\n",
        "\n",
        "https://github.com/openai/consistency_models/blob/e32b69ee436d518377db86fb2127a3972d0d8716/cm/unet.py#L478\n",
        "\n",
        "https://github.com/openai/guided-diffusion/blob/22e0df8183507e13a7813f8d38d51b072ca1e67c/guided_diffusion/unet.py#L361\n",
        "\n",
        "\n",
        "other:\n",
        "show attention: https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_ldm.ipynb\n",
        "\n",
        "\"cross_attention\"\n",
        "\n",
        "https://github.com/openai/point-e/blob/fc8a607c08a3ea804cc82bf1ef8628f88a3a5d2f/point_e/models/perceiver.py#L11\n",
        "\n",
        "https://github.com/openai/shap-e/blob/50131012ee11c9d2617f3886c10f000d3c7a3b43/shap_e/models/generation/perceiver.py#L13\n",
        "\n",
        "https://github.com/CompVis/latent-diffusion/blob/a506df5756472e2ebaf9078affdde2c4f1502cd4/ldm/modules/attention.py#L152\n",
        "\n",
        "https://blog.csdn.net/weixin_54338498/article/details/135051918\n",
        "\n",
        "\"attention_map\"\n",
        "https://github.com/google/prompt-to-prompt/tree/main\n"
      ],
      "metadata": {
        "id": "BIzPJV4YjsGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "juliew@neptune7:~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/models"
      ],
      "metadata": {
        "id": "wmyLS6XprfMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feedforward\n",
        "class GEGLU(nn.Module):\n",
        "    r\"\"\"\n",
        "    A variant of the gated linear unit activation function from https://arxiv.org/abs/2002.05202.\n",
        "\n",
        "    Parameters:\n",
        "        dim_in (`int`): The number of channels in the input.\n",
        "        dim_out (`int`): The number of channels in the output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_in: int, dim_out: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def gelu(self, gate):\n",
        "        if gate.device.type != \"mps\":\n",
        "            return F.gelu(gate)\n",
        "        # mps: gelu is not implemented for float16\n",
        "        return F.gelu(gate.to(dtype=torch.float32)).to(dtype=gate.dtype)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n",
        "        return hidden_states * self.gelu(gate)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    r\"\"\"\n",
        "    A feed-forward layer.\n",
        "\n",
        "    Parameters:\n",
        "        dim (`int`): The number of channels in the input.\n",
        "        dim_out (`int`, *optional*): The number of channels in the output. If not given, defaults to `dim`.\n",
        "        mult (`int`, *optional*, defaults to 4): The multiplier to use for the hidden dimension.\n",
        "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
        "        activation_fn (`str`, *optional*, defaults to `\"geglu\"`): Activation function to be used in feed-forward.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        dim_out: Optional[int] = None,\n",
        "        mult: int = 4,\n",
        "        dropout: float = 0.0,\n",
        "        activation_fn: str = \"geglu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = dim_out if dim_out is not None else dim\n",
        "\n",
        "        if activation_fn == \"gelu\":\n",
        "            act_fn = GELU(dim, inner_dim)\n",
        "        elif activation_fn == \"geglu\":\n",
        "            act_fn = GEGLU(dim, inner_dim)\n",
        "        elif activation_fn == \"geglu-approximate\":\n",
        "            act_fn = ApproximateGELU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.ModuleList([])\n",
        "        # project in\n",
        "        self.net.append(act_fn)\n",
        "        # project dropout\n",
        "        self.net.append(nn.Dropout(dropout))\n",
        "        # project out\n",
        "        self.net.append(nn.Linear(inner_dim, dim_out))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for module in self.net:\n",
        "            hidden_states = module(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    r\"\"\"\n",
        "    A cross attention layer.\n",
        "\n",
        "    Parameters:\n",
        "        query_dim (`int`): The number of channels in the query.\n",
        "        cross_attention_dim (`int`, *optional*):\n",
        "            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n",
        "        heads (`int`,  *optional*, defaults to 8): The number of heads to use for multi-head attention.\n",
        "        dim_head (`int`,  *optional*, defaults to 64): The number of channels in each head.\n",
        "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
        "        bias (`bool`, *optional*, defaults to False):\n",
        "            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        query_dim: int,\n",
        "        cross_attention_dim: Optional[int] = None,\n",
        "        heads: int = 8,\n",
        "        dim_head: int = 64,\n",
        "        dropout: float = 0.0,\n",
        "        bias=False,\n",
        "        upcast_attention: bool = False,\n",
        "        upcast_softmax: bool = False,\n",
        "        added_kv_proj_dim: Optional[int] = None,\n",
        "        norm_num_groups: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        cross_attention_dim = cross_attention_dim if cross_attention_dim is not None else query_dim\n",
        "        self.upcast_attention = upcast_attention\n",
        "        self.upcast_softmax = upcast_softmax\n",
        "\n",
        "        self.scale = dim_head**-0.5\n",
        "\n",
        "        self.heads = heads\n",
        "        # for slice_size > 0 the attention score computation\n",
        "        # is split across the batch axis to save memory\n",
        "        # You can set slice_size with `set_attention_slice`\n",
        "        self.sliceable_head_dim = heads\n",
        "        self._slice_size = None\n",
        "        self._use_memory_efficient_attention_xformers = False\n",
        "        self.added_kv_proj_dim = added_kv_proj_dim\n",
        "\n",
        "        if norm_num_groups is not None:\n",
        "            self.group_norm = nn.GroupNorm(num_channels=inner_dim, num_groups=norm_num_groups, eps=1e-5, affine=True)\n",
        "        else:\n",
        "            self.group_norm = None\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=bias)\n",
        "        self.to_k = nn.Linear(cross_attention_dim, inner_dim, bias=bias)\n",
        "        self.to_v = nn.Linear(cross_attention_dim, inner_dim, bias=bias)\n",
        "\n",
        "        if self.added_kv_proj_dim is not None:\n",
        "            self.add_k_proj = nn.Linear(added_kv_proj_dim, cross_attention_dim)\n",
        "            self.add_v_proj = nn.Linear(added_kv_proj_dim, cross_attention_dim)\n",
        "\n",
        "        self.to_out = nn.ModuleList([])\n",
        "        self.to_out.append(nn.Linear(inner_dim, query_dim))\n",
        "        self.to_out.append(nn.Dropout(dropout))\n",
        "\n",
        "    def reshape_heads_to_batch_dim(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.heads\n",
        "        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n",
        "        return tensor\n",
        "\n",
        "    def reshape_batch_dim_to_heads(self, tensor):\n",
        "        batch_size, seq_len, dim = tensor.shape\n",
        "        head_size = self.heads\n",
        "        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
        "        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n",
        "        return tensor\n",
        "\n",
        "    def set_attention_slice(self, slice_size):\n",
        "        if slice_size is not None and slice_size > self.sliceable_head_dim:\n",
        "            raise ValueError(f\"slice_size {slice_size} has to be smaller or equal to {self.sliceable_head_dim}.\")\n",
        "\n",
        "        self._slice_size = slice_size\n",
        "\n",
        "    def forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
        "        batch_size, sequence_length, _ = hidden_states.shape\n",
        "\n",
        "        encoder_hidden_states = encoder_hidden_states\n",
        "\n",
        "        if self.group_norm is not None:\n",
        "            hidden_states = self.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = self.to_q(hidden_states)\n",
        "        dim = query.shape[-1]\n",
        "        query = self.reshape_heads_to_batch_dim(query)\n",
        "\n",
        "        if self.added_kv_proj_dim is not None:\n",
        "            key = self.to_k(hidden_states)\n",
        "            value = self.to_v(hidden_states)\n",
        "            encoder_hidden_states_key_proj = self.add_k_proj(encoder_hidden_states)\n",
        "            encoder_hidden_states_value_proj = self.add_v_proj(encoder_hidden_states)\n",
        "\n",
        "            key = self.reshape_heads_to_batch_dim(key)\n",
        "            value = self.reshape_heads_to_batch_dim(value)\n",
        "            encoder_hidden_states_key_proj = self.reshape_heads_to_batch_dim(encoder_hidden_states_key_proj)\n",
        "            encoder_hidden_states_value_proj = self.reshape_heads_to_batch_dim(encoder_hidden_states_value_proj)\n",
        "\n",
        "            key = torch.concat([encoder_hidden_states_key_proj, key], dim=1)\n",
        "            value = torch.concat([encoder_hidden_states_value_proj, value], dim=1)\n",
        "        else:\n",
        "            encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n",
        "            key = self.to_k(encoder_hidden_states)\n",
        "            value = self.to_v(encoder_hidden_states)\n",
        "\n",
        "            key = self.reshape_heads_to_batch_dim(key)\n",
        "            value = self.reshape_heads_to_batch_dim(value)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.shape[-1] != query.shape[1]:\n",
        "                target_length = query.shape[1]\n",
        "                attention_mask = F.pad(attention_mask, (0, target_length), value=0.0)\n",
        "                attention_mask = attention_mask.repeat_interleave(self.heads, dim=0)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        if self._use_memory_efficient_attention_xformers:\n",
        "            hidden_states = self._memory_efficient_attention_xformers(query, key, value, attention_mask)\n",
        "            # Some versions of xformers return output in fp32, cast it back to the dtype of the input\n",
        "            hidden_states = hidden_states.to(query.dtype)\n",
        "        else:\n",
        "            if self._slice_size is None or query.shape[0] // self._slice_size == 1:\n",
        "                hidden_states = self._attention(query, key, value, attention_mask)\n",
        "            else:\n",
        "                hidden_states = self._sliced_attention(query, key, value, sequence_length, dim, attention_mask)\n",
        "\n",
        "        # linear proj\n",
        "        hidden_states = self.to_out[0](hidden_states)\n",
        "\n",
        "        # dropout\n",
        "        hidden_states = self.to_out[1](hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    def _attention(self, query, key, value, attention_mask=None):\n",
        "        if self.upcast_attention:\n",
        "            query = query.float()\n",
        "            key = key.float()\n",
        "\n",
        "        attention_scores = torch.baddbmm(\n",
        "            torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n",
        "            query,\n",
        "            key.transpose(-1, -2),\n",
        "            beta=0,\n",
        "            alpha=self.scale,\n",
        "        )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        if self.upcast_softmax:\n",
        "            attention_scores = attention_scores.float()\n",
        "\n",
        "        attention_probs = attention_scores.softmax(dim=-1)\n",
        "\n",
        "        # cast back to the original dtype\n",
        "        attention_probs = attention_probs.to(value.dtype)\n",
        "\n",
        "        # compute attention output\n",
        "        hidden_states = torch.bmm(attention_probs, value)\n",
        "\n",
        "        # reshape hidden_states\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    def _sliced_attention(self, query, key, value, sequence_length, dim, attention_mask):\n",
        "        batch_size_attention = query.shape[0]\n",
        "        hidden_states = torch.zeros(\n",
        "            (batch_size_attention, sequence_length, dim // self.heads), device=query.device, dtype=query.dtype\n",
        "        )\n",
        "        slice_size = self._slice_size if self._slice_size is not None else hidden_states.shape[0]\n",
        "        for i in range(hidden_states.shape[0] // slice_size):\n",
        "            start_idx = i * slice_size\n",
        "            end_idx = (i + 1) * slice_size\n",
        "\n",
        "            query_slice = query[start_idx:end_idx]\n",
        "            key_slice = key[start_idx:end_idx]\n",
        "\n",
        "            if self.upcast_attention:\n",
        "                query_slice = query_slice.float()\n",
        "                key_slice = key_slice.float()\n",
        "\n",
        "            attn_slice = torch.baddbmm(\n",
        "                torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query_slice.dtype, device=query.device),\n",
        "                query_slice,\n",
        "                key_slice.transpose(-1, -2),\n",
        "                beta=0,\n",
        "                alpha=self.scale,\n",
        "            )\n",
        "\n",
        "            if attention_mask is not None:\n",
        "                attn_slice = attn_slice + attention_mask[start_idx:end_idx]\n",
        "\n",
        "            if self.upcast_softmax:\n",
        "                attn_slice = attn_slice.float()\n",
        "\n",
        "            attn_slice = attn_slice.softmax(dim=-1)\n",
        "\n",
        "            # cast back to the original dtype\n",
        "            attn_slice = attn_slice.to(value.dtype)\n",
        "            attn_slice = torch.bmm(attn_slice, value[start_idx:end_idx])\n",
        "\n",
        "            hidden_states[start_idx:end_idx] = attn_slice\n",
        "\n",
        "        # reshape hidden_states\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    def _memory_efficient_attention_xformers(self, query, key, value, attention_mask):\n",
        "        # TODO attention_mask\n",
        "        query = query.contiguous()\n",
        "        key = key.contiguous()\n",
        "        value = value.contiguous()\n",
        "        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "WM3MFfYureF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "SZuCq7aw0CDr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwbzpXtUV85e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.self_attention"
      ],
      "metadata": {
        "id": "R_4jnLicHQCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1alg"
      ],
      "metadata": {
        "id": "XVp7sdZnF2tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "        #print(\"W_query shape:\", self.W_query)\n",
        "        #print(\"W_key shape:\", self.W_key)\n",
        "        #print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        #print(\"keys \", keys)\n",
        "        #print(\"queries \", queries)\n",
        "        #print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "EE1cZUpZ3RtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for MultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "batch_size = 1\n",
        "seq_len = 6\n",
        "\n",
        "multi_head_attention = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, sequence_length, d)).float()\n",
        "#print(\"MultiHeadAttentionWrapper input shape:\", x)\n",
        "output = multi_head_attention(x)\n",
        "#print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKmBdqOu33_8",
        "outputId": "767fde1d-ca82-47ba-8214-9c522fdce4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(A)\n",
        "print(A.shape)\n",
        "B=torch.tensor([[4,5,6],[7,8,9]])\n",
        "print(B)\n",
        "print(B.shape)\n",
        "print(torch.cat((A,B),dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFSHwJqbxMhW",
        "outputId": "644fad90-82cf-4356-fe06-477eb7e890e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "torch.Size([2, 3])\n",
            "tensor([[1, 2, 3, 4, 5, 6],\n",
            "        [4, 5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2two methods compare"
      ],
      "metadata": {
        "id": "g8eiLuUBF-uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the selfttention class from Method 1\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "        #print(\"W_query shape:\", self.W_query)\n",
        "        #print(\"W_key shape:\", self.W_key)\n",
        "        #print(\"W_value shape:\", self.W_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "        print(\"queries \", queries)\n",
        "        print(\"keys \", keys)\n",
        "        print(\"values \", values)\n",
        "        attn_scores = queries @ keys.transpose(-2,-1)  # unnormalized attention weights\n",
        "        #print(\"attn_scores \", attn_scores )\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        #print(\"attn_weights \", attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        #print(\"context_vec \", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [SelfAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        #print(\"heads \", self.heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class QKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        print(\"q\", q, q.shape)\n",
        "        print(\"k\", k.shape)\n",
        "        print(\"v\", v.shape )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 1\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# Example input\n",
        "x = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input shape:\", x)\n",
        "\n",
        "# Initialize Method 1\n",
        "multihead_attention_1 = MultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output_1 = multihead_attention_1(x)\n",
        "\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x @ head.W_query for head in multihead_attention_1.heads]\n",
        "keys = [x @ head.W_key for head in multihead_attention_1.heads]\n",
        "values = [x @ head.W_value for head in multihead_attention_1.heads]\n",
        "print(\"queries before cat \", queries)\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1) )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv  )\n",
        "\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "qkv_attention = QKVAttention(num_heads)\n",
        "output_2 = qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1, output_1.shape)\n",
        "print(\"Output from Method 2:\\n\", output_2, output_2.shape)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZzKWxwBd-Y",
        "outputId": "bbd5727a-7915-4433-f333-024fba81e183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttentionWrapper input shape: tensor([[[1., 2., 1., 2.],\n",
            "         [2., 2., 3., 1.],\n",
            "         [2., 0., 3., 3.],\n",
            "         [2., 2., 2., 1.],\n",
            "         [1., 0., 2., 2.],\n",
            "         [2., 0., 1., 0.]]])\n",
            "queries  tensor([[[14., 13.],\n",
            "         [19., 17.],\n",
            "         [17., 21.],\n",
            "         [16., 15.],\n",
            "         [11., 13.],\n",
            "         [ 5.,  8.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys  tensor([[[16., 11.],\n",
            "         [20.,  9.],\n",
            "         [20., 11.],\n",
            "         [17.,  9.],\n",
            "         [13.,  7.],\n",
            "         [ 5.,  2.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values  tensor([[[ 6., 15.],\n",
            "         [ 5., 18.],\n",
            "         [ 3., 18.],\n",
            "         [ 5., 15.],\n",
            "         [ 2., 12.],\n",
            "         [ 0.,  3.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "queries before cat  [tensor([[[14., 13.],\n",
            "         [19., 17.],\n",
            "         [17., 21.],\n",
            "         [16., 15.],\n",
            "         [11., 13.],\n",
            "         [ 5.,  8.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "queries after cat tensor([[[14., 13.],\n",
            "         [19., 17.],\n",
            "         [17., 21.],\n",
            "         [16., 15.],\n",
            "         [11., 13.],\n",
            "         [ 5.,  8.]]], grad_fn=<CatBackward0>)\n",
            "before permute qkv tensor([[[14., 13., 16., 11.,  6., 15.],\n",
            "         [19., 17., 20.,  9.,  5., 18.],\n",
            "         [17., 21., 20., 11.,  3., 18.],\n",
            "         [16., 15., 17.,  9.,  5., 15.],\n",
            "         [11., 13., 13.,  7.,  2., 12.],\n",
            "         [ 5.,  8.,  5.,  2.,  0.,  3.]]], grad_fn=<CatBackward0>)\n",
            "after permute qkv tensor([[[14., 19., 17., 16., 11.,  5.],\n",
            "         [13., 17., 21., 15., 13.,  8.],\n",
            "         [16., 20., 20., 17., 13.,  5.],\n",
            "         [11.,  9., 11.,  9.,  7.,  2.],\n",
            "         [ 6.,  5.,  3.,  5.,  2.,  0.],\n",
            "         [15., 18., 18., 15., 12.,  3.]]], grad_fn=<PermuteBackward0>)\n",
            "q tensor([[[14., 19., 17., 16., 11.,  5.],\n",
            "         [13., 17., 21., 15., 13.,  8.]]], grad_fn=<SplitBackward0>) torch.Size([1, 2, 6])\n",
            "k torch.Size([1, 2, 6])\n",
            "v torch.Size([1, 2, 6])\n",
            "Output from Method 1:\n",
            " tensor([[[ 3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000],\n",
            "         [18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000]]],\n",
            "       grad_fn=<PermuteBackward0>) torch.Size([1, 2, 6])\n",
            "Output from Method 2:\n",
            " tensor([[[ 3.0000,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000],\n",
            "         [18.0000, 18.0000, 18.0000, 18.0000, 18.0000, 18.0000]]],\n",
            "       grad_fn=<ReshapeAliasBackward0>) torch.Size([1, 2, 6])\n",
            "Are the outputs close?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3attention_map\n",
        "https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb"
      ],
      "metadata": {
        "id": "om-WM61h0Bfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.CrossAttention"
      ],
      "metadata": {
        "id": "dJRtOrM46kkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1alg"
      ],
      "metadata": {
        "id": "D2JBgzJNHJvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "2lCfxcpB7smg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for CrossMultiHeadAttentionWrapper\n",
        "d = 3\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "batch_size = 1\n",
        "seq_len = 6\n",
        "\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"MultiHeadAttentionWrapper input1 shape:\", x_1)\n",
        "print(\"MultiHeadAttentionWrapper input2 shape:\", x_2)\n",
        "output = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "#output shape [batch_size, seq_len, num_heads * d_v]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZcqEqSWIxep",
        "outputId": "31584e3c-f3e9-489c-c19d-222d908a8ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "MultiHeadAttentionWrapper input1 shape: tensor([[[0., 1., 1.],\n",
            "         [1., 1., 2.],\n",
            "         [2., 0., 2.],\n",
            "         [0., 2., 1.],\n",
            "         [2., 1., 1.],\n",
            "         [2., 1., 0.]]])\n",
            "MultiHeadAttentionWrapper input2 shape: tensor([[[1., 1., 2.],\n",
            "         [1., 1., 1.],\n",
            "         [0., 2., 0.],\n",
            "         [0., 1., 1.],\n",
            "         [0., 2., 2.],\n",
            "         [0., 0., 2.]]])\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[3.3459, 7.3445, 4.0000, 4.0000],\n",
            "         [3.9441, 7.9441, 4.0000, 4.0000],\n",
            "         [3.9861, 7.9861, 4.0000, 4.0000],\n",
            "         [3.3457, 7.3454, 4.0000, 4.0000],\n",
            "         [3.9441, 7.9441, 4.0000, 4.0000],\n",
            "         [3.7887, 7.7870, 3.9999, 3.9999]]], grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 two methods compare"
      ],
      "metadata": {
        "id": "BZcr1G8ZKe2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the QKVAttention class from Method 1\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d, d_k_q, d_v):\n",
        "        super().__init__()\n",
        "        self.d_out_kq = d_k_q\n",
        "        self.W_query = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_key   = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_k_q)).float())\n",
        "        self.W_value = nn.Parameter(torch.randint(low=0, high=d, size=(d, d_v)).float())\n",
        "\n",
        "    def forward(self, x_1, x_2):           # x_2 is new\n",
        "        queries_1 = x_1 @ self.W_query\n",
        "\n",
        "        keys_2 = x_2 @ self.W_key          # new\n",
        "        values_2 = x_2 @ self.W_value      # new\n",
        "        print(\"queries_1 shape:\", queries_1)\n",
        "        print(\"keys_2 shape:\", keys_2)\n",
        "        print(\"values_2 shape:\", values_2)\n",
        "        attn_scores = queries_1 @ keys_2.transpose(-2, -1) # new\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values_2\n",
        "        return context_vec\n",
        "\n",
        "class CrossMultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d, d_k_q, d_v, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CrossAttention(d, d_k_q, d_v)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "        print(\"heads \", dir(self.heads[0]))\n",
        "\n",
        "    def forward(self, x_1, x_2):\n",
        "        return torch.cat([head(x_1, x_2) for head in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "# Define the QKVAttention class from Method 2\n",
        "class CrossQKVAttention(nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.chunk(3, dim=1)\n",
        "        print(\"q\", q, q.shape)\n",
        "        print(\"k\", k.shape)\n",
        "        print(\"v\", v.shape )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = torch.einsum(\n",
        "            \"bct,bcs->bts\",\n",
        "            (q * scale).view(bs * self.n_heads, ch, length),\n",
        "            (k * scale).view(bs * self.n_heads, ch, length),\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = torch.einsum(\n",
        "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
        "        )\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "# Example parameters\n",
        "# Example parameters\n",
        "d = 4\n",
        "d_k_q = 2\n",
        "d_v = 2\n",
        "num_heads = 2\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# Example input\n",
        "x_1 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "x_2 = torch.randint(low=0,high=d,size=(batch_size, seq_len, d)).float()\n",
        "print(\"CrossMultiHeadAttentionWrapper input 1 shape:\", x_1)\n",
        "print(\"CrossMultiHeadAttentionWrapper input 2 shape:\", x_2)\n",
        "\n",
        "# Initialize Method 1\n",
        "cross_multi_head_attention = CrossMultiHeadAttentionWrapper(d, d_k_q, d_v, num_heads)\n",
        "output_1 = cross_multi_head_attention(x_1,x_2)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\", output)\n",
        "print(\"MultiHeadAttentionWrapper output shape:\" , output.shape )\n",
        "\n",
        "print(\"-----------------------------------------------------\")\n",
        "# Initialize Method 2\n",
        "# For Method 2, we need to prepare qkv input\n",
        "# We'll use the same weights as in Method 1 to ensure identical outputs\n",
        "queries = [x_1 @ head.W_query for head in cross_multi_head_attention.heads]\n",
        "keys = [x_2 @ head.W_key for head in cross_multi_head_attention.heads]\n",
        "values = [x_2 @ head.W_value for head in cross_multi_head_attention.heads]\n",
        "print(\"queries before cat \", queries)\n",
        "print(\"keys before cat \", keys)\n",
        "print(\"values before cat \", values)\n",
        "\n",
        "\n",
        "# Concatenate Q, K, V for Method 2 input\n",
        "queries = torch.cat(queries, dim=-1)\n",
        "keys = torch.cat(keys, dim=-1)\n",
        "values = torch.cat(values, dim=-1)\n",
        "print(\"queries after cat\", queries)\n",
        "\n",
        "# Stack the heads to prepare qkv input\n",
        "print(\"before permute qkv\", torch.cat((queries, keys, values), dim=-1) )\n",
        "qkv = torch.cat((queries, keys, values), dim=-1).permute(0, 2, 1)\n",
        "print(\"after permute qkv\", qkv  )\n",
        "\n",
        "# Initialize and run QKVAttention\n",
        "cross_qkv_attention = CrossQKVAttention(num_heads)\n",
        "output_2 = cross_qkv_attention(qkv)\n",
        "#output shape [batch_size, num_heads * d_v,seq_len], here d_out_v = d_k_q\n",
        "\n",
        "# Permute output_1 to match output_2 shape\n",
        "output_1 = output_1.permute(0, 2, 1)\n",
        "\n",
        "# Compare the outputs\n",
        "print(\"Output from Method 1:\\n\", output_1)\n",
        "print(\"Output from Method 2:\\n\", output_2)\n",
        "\n",
        "# Check if the outputs are close enough\n",
        "print(\"Are the outputs close? \", torch.allclose(output_1, output_2, atol=1e-6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oxGKS6nKmOb",
        "outputId": "8c9a7749-084b-4a32-de7c-7000087efc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossMultiHeadAttentionWrapper input 1 shape: tensor([[[2., 1., 2., 2.],\n",
            "         [1., 2., 0., 2.],\n",
            "         [1., 2., 3., 1.],\n",
            "         [1., 2., 1., 1.],\n",
            "         [2., 1., 0., 1.],\n",
            "         [0., 2., 2., 0.]]])\n",
            "CrossMultiHeadAttentionWrapper input 2 shape: tensor([[[2., 1., 0., 3.],\n",
            "         [2., 1., 2., 3.],\n",
            "         [1., 1., 1., 0.],\n",
            "         [2., 1., 3., 0.],\n",
            "         [3., 2., 1., 2.],\n",
            "         [1., 3., 1., 3.]]])\n",
            "heads  ['T_destination', 'W_key', 'W_query', 'W_value', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'd_out_kq', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "queries_1 shape: tensor([[[10., 15.],\n",
            "         [10.,  9.],\n",
            "         [10., 15.],\n",
            "         [ 8.,  9.],\n",
            "         [ 5.,  6.],\n",
            "         [ 6.,  8.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[12., 11.],\n",
            "         [18., 15.],\n",
            "         [ 5.,  3.],\n",
            "         [12.,  8.],\n",
            "         [14., 11.],\n",
            "         [16., 12.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[ 5., 11.],\n",
            "         [11., 13.],\n",
            "         [ 6.,  5.],\n",
            "         [14.,  8.],\n",
            "         [11., 14.],\n",
            "         [ 8., 17.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "queries_1 shape: tensor([[[1., 7.],\n",
            "         [2., 5.],\n",
            "         [2., 7.],\n",
            "         [2., 5.],\n",
            "         [1., 4.],\n",
            "         [2., 4.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "keys_2 shape: tensor([[[10., 11.],\n",
            "         [14., 13.],\n",
            "         [ 7.,  3.],\n",
            "         [13.,  5.],\n",
            "         [16., 11.],\n",
            "         [16., 16.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "values_2 shape: tensor([[[ 8., 12.],\n",
            "         [12., 16.],\n",
            "         [ 7.,  5.],\n",
            "         [14.,  9.],\n",
            "         [15., 14.],\n",
            "         [11., 20.]]], grad_fn=<UnsafeViewBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: tensor([[[10.0000, 21.0000],\n",
            "         [10.0000, 21.0000],\n",
            "         [ 9.9998, 20.9992],\n",
            "         [10.0000, 21.0000],\n",
            "         [10.0000, 21.0000],\n",
            "         [10.0000, 21.0000]]], grad_fn=<CatBackward0>)\n",
            "MultiHeadAttentionWrapper output shape: torch.Size([1, 6, 2])\n",
            "-----------------------------------------------------\n",
            "queries before cat  [tensor([[[10., 15.],\n",
            "         [10.,  9.],\n",
            "         [10., 15.],\n",
            "         [ 8.,  9.],\n",
            "         [ 5.,  6.],\n",
            "         [ 6.,  8.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[1., 7.],\n",
            "         [2., 5.],\n",
            "         [2., 7.],\n",
            "         [2., 5.],\n",
            "         [1., 4.],\n",
            "         [2., 4.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "keys before cat  [tensor([[[12., 11.],\n",
            "         [18., 15.],\n",
            "         [ 5.,  3.],\n",
            "         [12.,  8.],\n",
            "         [14., 11.],\n",
            "         [16., 12.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[10., 11.],\n",
            "         [14., 13.],\n",
            "         [ 7.,  3.],\n",
            "         [13.,  5.],\n",
            "         [16., 11.],\n",
            "         [16., 16.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "values before cat  [tensor([[[ 5., 11.],\n",
            "         [11., 13.],\n",
            "         [ 6.,  5.],\n",
            "         [14.,  8.],\n",
            "         [11., 14.],\n",
            "         [ 8., 17.]]], grad_fn=<UnsafeViewBackward0>), tensor([[[ 8., 12.],\n",
            "         [12., 16.],\n",
            "         [ 7.,  5.],\n",
            "         [14.,  9.],\n",
            "         [15., 14.],\n",
            "         [11., 20.]]], grad_fn=<UnsafeViewBackward0>)]\n",
            "queries after cat tensor([[[10., 15.,  1.,  7.],\n",
            "         [10.,  9.,  2.,  5.],\n",
            "         [10., 15.,  2.,  7.],\n",
            "         [ 8.,  9.,  2.,  5.],\n",
            "         [ 5.,  6.,  1.,  4.],\n",
            "         [ 6.,  8.,  2.,  4.]]], grad_fn=<CatBackward0>)\n",
            "before permute qkv tensor([[[10., 15.,  1.,  7., 12., 11., 10., 11.,  5., 11.,  8., 12.],\n",
            "         [10.,  9.,  2.,  5., 18., 15., 14., 13., 11., 13., 12., 16.],\n",
            "         [10., 15.,  2.,  7.,  5.,  3.,  7.,  3.,  6.,  5.,  7.,  5.],\n",
            "         [ 8.,  9.,  2.,  5., 12.,  8., 13.,  5., 14.,  8., 14.,  9.],\n",
            "         [ 5.,  6.,  1.,  4., 14., 11., 16., 11., 11., 14., 15., 14.],\n",
            "         [ 6.,  8.,  2.,  4., 16., 12., 16., 16.,  8., 17., 11., 20.]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "after permute qkv tensor([[[10., 10., 10.,  8.,  5.,  6.],\n",
            "         [15.,  9., 15.,  9.,  6.,  8.],\n",
            "         [ 1.,  2.,  2.,  2.,  1.,  2.],\n",
            "         [ 7.,  5.,  7.,  5.,  4.,  4.],\n",
            "         [12., 18.,  5., 12., 14., 16.],\n",
            "         [11., 15.,  3.,  8., 11., 12.],\n",
            "         [10., 14.,  7., 13., 16., 16.],\n",
            "         [11., 13.,  3.,  5., 11., 16.],\n",
            "         [ 5., 11.,  6., 14., 11.,  8.],\n",
            "         [11., 13.,  5.,  8., 14., 17.],\n",
            "         [ 8., 12.,  7., 14., 15., 11.],\n",
            "         [12., 16.,  5.,  9., 14., 20.]]], grad_fn=<PermuteBackward0>)\n",
            "q tensor([[[10., 10., 10.,  8.,  5.,  6.],\n",
            "         [15.,  9., 15.,  9.,  6.,  8.],\n",
            "         [ 1.,  2.,  2.,  2.,  1.,  2.],\n",
            "         [ 7.,  5.,  7.,  5.,  4.,  4.]]], grad_fn=<SplitBackward0>) torch.Size([1, 4, 6])\n",
            "k torch.Size([1, 4, 6])\n",
            "v torch.Size([1, 4, 6])\n",
            "Output from Method 1:\n",
            " tensor([[[11.0000, 11.0000, 11.0000, 11.0000, 11.0000, 11.0000],\n",
            "         [13.0000, 13.0000, 13.0000, 13.0000, 13.0000, 13.0000],\n",
            "         [11.0000, 11.0000, 11.0000, 11.0000, 11.0001, 11.0000],\n",
            "         [20.0000, 20.0000, 20.0000, 20.0000, 19.9998, 19.9999]]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "Output from Method 2:\n",
            " tensor([[[11.0000, 11.0000, 11.0000, 11.0000, 11.0000, 11.0000],\n",
            "         [13.0000, 13.0000, 13.0000, 13.0000, 13.0000, 13.0000],\n",
            "         [11.0000, 11.0000, 11.0000, 11.0000, 11.0001, 11.0000],\n",
            "         [20.0000, 20.0000, 20.0000, 20.0000, 19.9998, 19.9999]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Are the outputs close?  True\n"
          ]
        }
      ]
    }
  ]
}